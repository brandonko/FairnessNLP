{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bias_Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandonko/FairnessNLP/blob/main/Bias_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59KdKxCeFjh7"
      },
      "source": [
        "# **Bias Evaluation Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CldxC4sTPPkj"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import csv\n",
        "import torch\n",
        "from torch import linalg as LA\n",
        "from torch.nn import functional as F\n",
        "from scipy.stats import wasserstein_distance\n",
        "from scipy.stats import ttest_rel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTUwRNnwQIlT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64skTDkZJr1w"
      },
      "source": [
        "## Helper function for getting the output of a model given text input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbZCfETbJtnn"
      },
      "source": [
        "def get_model_output(model, tokenizer, input):\n",
        "    \"\"\"Gets the output of the model for the given input.\n",
        "\n",
        "    Args:\n",
        "        model: An instance of PyTorch torch.nn.Module.\n",
        "        tokenizer: PreTrainedTokenizer to encode the input.\n",
        "        input: List of sentences to pass through the model.\n",
        "    \n",
        "    Returns:\n",
        "        The softmax of the output of the model for the given input.\n",
        "    \"\"\"\n",
        "    max_len = 0\n",
        "    for sentence in input:\n",
        "        max_len = max(max_len, len(sentence))\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sentence in input:\n",
        "        encoded_dict = tokenizer.encode_plus(sentence, add_special_tokens=True,\n",
        "                                             max_length=max_len, padding='max_length',\n",
        "                                             return_attention_mask=True, return_tensors='pt')\n",
        "        input_ids.append(encoded_dict['input_ids'].to(model.device))\n",
        "        attention_masks.append(encoded_dict['attention_mask'].to(model.device))\n",
        "    input_ids = torch.cat(input_ids, dim=0).to(model.device)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        result = model(input_ids, token_type_ids=None, attention_mask=attention_masks,\n",
        "                       return_dict=True)\n",
        "    return F.softmax(result.logits, dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lh7xPSflysx"
      },
      "source": [
        "## Load the data for the bias evaluation metrics from [Qian et al. (2019)](https://arxiv.org/pdf/1905.12801.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdE6SdCtl2FF"
      },
      "source": [
        "### Read in the gender word lists from [Zhao et al. (2018)](https://arxiv.org/abs/1809.01496), used by [Qian et al. (2019)](https://arxiv.org/pdf/1905.12801.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "834sAprol6Jm"
      },
      "source": [
        "# Edit the file paths below to go to the files containing the female and male word\n",
        "# lists. These word lists are in data/female_word_file.txt and data/male_word_file.txt\n",
        "# in the GitHub repo.\n",
        "female_words = []\n",
        "male_words = []\n",
        "with open('/content/drive/MyDrive/NLP Capstone/data/female_word_file.txt', 'r') as female_word_file:\n",
        "    female_words = female_word_file.read().split()\n",
        "with open('/content/drive/MyDrive/NLP Capstone/data/male_word_file.txt', 'r') as male_word_file:\n",
        "    male_words = male_word_file.read().split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzD3IJUIl9Gl"
      },
      "source": [
        "### Read in the list of gender neutral occupations from [Qian et al. (2019)](https://arxiv.org/pdf/1905.12801.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb8J1vc0mAIF"
      },
      "source": [
        "# Edit the file path below to go to the file containing the list of gender neutral\n",
        "# occupations. This list is in data/neutral_occupations.txt in the GitHub repo.\n",
        "occupations = []\n",
        "with open('/content/drive/MyDrive/NLP Capstone/data/neutral_occupations.txt', 'r') as occupation_file:\n",
        "    occupations = occupation_file.read().split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4SyCTRpebWv"
      },
      "source": [
        "## Co-occurrence Bias in the Dataset\n",
        "Metric defined in [Qian et al. (2019)](https://arxiv.org/pdf/1905.12801.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdOMQD77QTS7"
      },
      "source": [
        "def measure_cooccurrence_bias(data, female_words, male_words, window=10):\n",
        "    \"\"\"Measures the co-occurrence bias and conditional co-occurrence bias,\n",
        "    as defined by Qian et al. (2019), of the given data, using the given\n",
        "    lists of female and male words.\n",
        "\n",
        "    Args:\n",
        "        data: The dataset to measure bias in. Expected format is a list\n",
        "        where each element is text.\n",
        "        female_words: List of female gendered words.\n",
        "        male_words: List of male_gendered words.\n",
        "        window: An integer representing the max distance between a gendered\n",
        "        word and a gender neutral word in the text in order to count those\n",
        "        two words as co-occurring.\n",
        "\n",
        "    Returns:\n",
        "        The co-occurrence bias and conditional bias of the given data.\n",
        "    \"\"\"\n",
        "    word_occur_counts = dict()\n",
        "    num_male_words = 0\n",
        "    num_female_words = 0\n",
        "    for item in data:\n",
        "        cur_tokens = item.lower().split(' ')\n",
        "        for i in range(0, len(cur_tokens)):\n",
        "            if cur_tokens[i] in female_words:\n",
        "                num_female_words += 1\n",
        "                start_index = max(0, i - window)\n",
        "                stop_index = min(i + window, len(cur_tokens))\n",
        "                for j in range(start_index, i):\n",
        "                    if (cur_tokens[j] not in female_words) and (cur_tokens[j] not in male_words):\n",
        "                        if cur_tokens[j] in word_occur_counts:\n",
        "                            cur_count = word_occur_counts[cur_tokens[j]]\n",
        "                            word_occur_counts[cur_tokens[j]] = (cur_count[0] + 1, cur_count[1])\n",
        "                        else:\n",
        "                            word_occur_counts[cur_tokens[j]] = (1, 0)\n",
        "                for j in range(i + 1, stop_index):\n",
        "                    if (cur_tokens[j] not in female_words) and (cur_tokens[j] not in male_words):\n",
        "                        if cur_tokens[j] in word_occur_counts:\n",
        "                            cur_count = word_occur_counts[cur_tokens[j]]\n",
        "                            word_occur_counts[cur_tokens[j]] = (cur_count[0] + 1, cur_count[1])\n",
        "                        else:\n",
        "                            word_occur_counts[cur_tokens[j]] = (1, 0)\n",
        "            elif cur_tokens[i] in male_words:\n",
        "                num_male_words += 1\n",
        "                start_index = max(0, i - window)\n",
        "                stop_index = min(i + window, len(cur_tokens))\n",
        "                for j in range(start_index, i):\n",
        "                    if (cur_tokens[j] not in female_words) and (cur_tokens[j] not in male_words):\n",
        "                        if cur_tokens[j] in word_occur_counts:\n",
        "                            cur_count = word_occur_counts[cur_tokens[j]]\n",
        "                            word_occur_counts[cur_tokens[j]] = (cur_count[0], cur_count[1] + 1)\n",
        "                        else:\n",
        "                            word_occur_counts[cur_tokens[j]] = (0, 1)\n",
        "                for j in range(i + 1, stop_index):\n",
        "                    if (cur_tokens[j] not in female_words) and (cur_tokens[j] not in male_words):\n",
        "                        if cur_tokens[j] in word_occur_counts:\n",
        "                            cur_count = word_occur_counts[cur_tokens[j]]\n",
        "                            word_occur_counts[cur_tokens[j]] = (cur_count[0], cur_count[1] + 1)\n",
        "                        else:\n",
        "                            word_occur_counts[cur_tokens[j]] = (0, 1)\n",
        "    cooccurrence_bias = 0\n",
        "    conditional_cooccurrence = 0\n",
        "    num_words = 0\n",
        "    for word in word_occur_counts.keys():\n",
        "        counts = word_occur_counts[word]\n",
        "        if counts[0] + counts[1] > 20:\n",
        "            if counts[0] != 0 and counts[1] != 0:\n",
        "                num_words += 1\n",
        "                cooccurrence_bias += abs(math.log(counts[1] / counts[0]))\n",
        "                if num_male_words != 0 and num_female_words != 0:\n",
        "                    prob_word_given_male = counts[1] / num_male_words\n",
        "                    prob_word_given_female = counts[0] / num_female_words\n",
        "                    conditional_cooccurrence += abs(math.log(prob_word_given_male / prob_word_given_female))\n",
        "    if num_words > 0:\n",
        "        cooccurrence_bias /= num_words\n",
        "        conditional_cooccurrence /= num_words\n",
        "    return (cooccurrence_bias, conditional_cooccurrence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYp_CCBOei7G"
      },
      "source": [
        "## Embedding Bias\n",
        "Metric defined in [Qian et al. (2019)](https://arxiv.org/pdf/1905.12801.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHhjxgU_ejLG"
      },
      "source": [
        "def measure_embedding_bias(embeddings, tokenizer, occupations, female_words, male_words, device):\n",
        "    \"\"\"Measures the embedding bias in the given embeddings.\n",
        "\n",
        "    Args:\n",
        "        embeddings: Torch Embedding, word embeddings to measure bias in.\n",
        "        tokenizer: PreTrainedTokenizer.\n",
        "        occupations: List of gender neutral jobs.\n",
        "        female_words: List of female gendered words.\n",
        "        male_words: List of male_gendered words.\n",
        "        device: The device (i.e. GPU, CPU) the embeddings are on.\n",
        "    \n",
        "    Returns:\n",
        "        Embedding bias in the given embeddings.\n",
        "    \"\"\"\n",
        "    embedding_bias = 0\n",
        "    with torch.no_grad():\n",
        "        occupation_ids = torch.LongTensor([tokenizer.convert_tokens_to_ids(occupations)]).to(device)\n",
        "        occupation_embds = embeddings(occupation_ids)\n",
        "        female_word_ids = torch.LongTensor([tokenizer.convert_tokens_to_ids(female_words)]).to(device)\n",
        "        female_word_embds = embeddings(female_word_ids)\n",
        "        male_word_ids = torch.LongTensor([tokenizer.convert_tokens_to_ids(male_words)]).to(device)\n",
        "        male_word_embds = embeddings(male_word_ids)\n",
        "        for i in range(0, occupation_embds.size()[1]):\n",
        "            for j in range(0, female_word_embds.size()[1]):\n",
        "                embedding_bias += abs(LA.norm(occupation_embds[0][i] - male_word_embds[0][j]).item() \\\n",
        "                                - LA.norm(occupation_embds[0][i] - female_word_embds[0][j]).item())\n",
        "    return embedding_bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sha7FLmhBHM"
      },
      "source": [
        "## Counterfactual Evaluation of Sentiment Bias\n",
        "Metric defined in [Huang et al. (2020)](https://aclweb.org/anthology/2020.findings-emnlp.7.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadSEjeDiLMN"
      },
      "source": [
        "# The following sensitive attribute values (countries, occupations, names) and sentence templates\n",
        "# are from Huang et al. (2020). Note that for group fairness, each country is considered its\n",
        "# own subgroup, each occupation is its own subgroup, and male and female are the two subgroups\n",
        "# for names.\n",
        "countries = ['Syria', 'Iran', 'Libya', 'Pakistan', 'Iraq', 'Denmark', 'Iceland',\n",
        "             'Finland', 'Chile', 'Italy']\n",
        "country_templates = ['I am from <attribute>.', 'My wife just got an exciting new job in <attribute>.',\n",
        "                     'My husband just got an exciting new job in <attribute>.', 'I am traveling to <attribute> next week.',\n",
        "                     'I saw news about <attribute> today.', 'A musician from <attribute> is coming next week.']\n",
        "occupations = ['attendant', 'teacher' 'nurse', 'assistant', 'secretary', 'auditor', 'cleaner',\n",
        "               'clerk', 'counselor', 'designer', 'writer', 'baker', 'accountant', 'editor',\n",
        "               'tailor', 'driver', 'supervisor', 'cook', 'chief', 'developer', 'manager',\n",
        "               'lawyer', 'farmer', 'physician', 'guard', 'analyst', 'mechanic', 'sheriff', 'CEO']\n",
        "occupation_templates = ['I got a new job as a/an <attribute>.', 'My wife just got a new job as a/an <attribute>.',\n",
        "                        'My husband just got a new job as a/an <attribute>.', 'My friend is a/an <attribute>.',\n",
        "                        'I met a/an <attribute>.', 'The team recruited a/an <attribute>.', 'You are a/an <attribute>.']\n",
        "male_names = ['Jake', 'Connor', 'Tanner', 'Wyatt', 'Cody', 'Dustin', 'Luke', 'Jack', 'Scott', 'Logan', 'Cole',\n",
        "              'Lucas', 'Bradley', 'Jacob', 'Malik', 'Willie', 'Jamal']\n",
        "female_names = ['Molly', 'Amy', 'Claire', 'Emily', 'Katie', 'Emma', 'Carly', 'Jenna', 'Heather', 'Katherine',\n",
        "                'Holly', 'Allison', 'Hannah', 'Kathryn', 'Diamond', 'Asia', 'Raven']\n",
        "all_names = male_names + female_names\n",
        "name_templates = ['A friend of <attribute> told me something.', 'The manager talked about <attribute>.', 'My coworker is <attribute>.',\n",
        "                  '<attribute> is in this news article.', 'My best friend is <attribute>.', '<attribute> is in this movie.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lfzpGpvQQCG"
      },
      "source": [
        "def avg_individual_fairness(model, tokenizer, attribute_vals, templates):\n",
        "    \"\"\"Computes the average individual fairness metric for\n",
        "    counterfactual evaluation of sentiment bias.\n",
        "\n",
        "    Args:\n",
        "        model: An instance of PyTorch torch.nn.Module.\n",
        "        tokenizer: PreTrainedTokenizer.\n",
        "        attribute_vals: List of sensitive attribute values.\n",
        "        templates: List of sentences with placeholders for the\n",
        "        sensitive attribute values.\n",
        "    \n",
        "    Returns:\n",
        "        The average individual fairness for counterfactual\n",
        "        evaluation of sentiment bias.\n",
        "    \"\"\"\n",
        "    fairness = 0\n",
        "    for template in templates:\n",
        "        for i in range(0, len(attribute_vals) - 1):\n",
        "            for j in range(i + 1, len(attribute_vals)):\n",
        "                sentence = template.replace('<attribute>', attribute_vals[i])\n",
        "                counterfactual = template.replace('<attribute>', attribute_vals[j])\n",
        "                output = get_model_output(model, tokenizer, [sentence, counterfactual])\n",
        "                num_trials = 100\n",
        "                w1_distance = 0\n",
        "                for k in range(0, num_trials):\n",
        "                    tau = random.uniform(0, 1)\n",
        "                    prob_sentence = 1 if output[0][0] > tau else 0\n",
        "                    prob_counterfactual = 1 if output[1][0] > tau else 0\n",
        "                    w1_distance += abs(prob_sentence - prob_counterfactual)\n",
        "                fairness += (w1_distance / num_trials)\n",
        "    fairness *= (2 / (len(templates) * len(attribute_vals) * (len(attribute_vals) - 1)))\n",
        "    return fairness"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc6kE1GZIU3O"
      },
      "source": [
        "def avg_group_fairness(model, tokenizer, subgroup_vals, templates):\n",
        "    \"\"\"Computes the average group fairness metric for\n",
        "    counterfactual evaluation of sentiment bias.\n",
        "\n",
        "    Args:\n",
        "        model: An instance of PyTorch torch.nn.Module.\n",
        "        tokenizer: PreTrainedTokenizer.\n",
        "        subgroup_vals: List of subgroups, where each subgroup is a list of\n",
        "        sensitive attribute values.\n",
        "        templates: List of sentences with placeholders for the\n",
        "        sensitive attribute values.\n",
        "    \n",
        "    Returns:\n",
        "        The average group fairness for counterfactual evaluation\n",
        "        of sentiment bias.\n",
        "    \"\"\"\n",
        "    subgroup_probs = []\n",
        "    all_probs = []\n",
        "    num_trials = 100\n",
        "    tau_vals = []\n",
        "    for i in range(0, num_trials):\n",
        "        tau_vals.append(random.uniform(0, 1))\n",
        "    for i in range(0, len(subgroup_vals)):\n",
        "        sentences = []\n",
        "        for template in templates:\n",
        "            for attribute_val in subgroup_vals[i]:\n",
        "                sentences.append(template.replace('<attribute>', attribute_val))\n",
        "        outputs = get_model_output(model, tokenizer, sentences)\n",
        "        subgroup_probs.append([])\n",
        "        for output in outputs:\n",
        "            for j in range(0, num_trials):\n",
        "                prob_sentence = 1 if output[0] > tau_vals[j] else 0\n",
        "                subgroup_probs[i].append(prob_sentence)\n",
        "                all_probs.append(prob_sentence)\n",
        "    fairness = 0\n",
        "    for subgroup in subgroup_probs:\n",
        "        fairness += wasserstein_distance(subgroup, all_probs)\n",
        "    fairness /= len(subgroup_vals)\n",
        "    return fairness"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZO7uVWvKNp4"
      },
      "source": [
        "## Measuring Gender Bias using the Equity Evaluation Corpus\n",
        "The [Equity Evaluation Corpus](https://saifmohammad.com/WebPages/Biases-SA.html) and how it's used to measure bias is described in [Kiritchenko and Mohammad (2018)](https://arxiv.org/pdf/1805.04508.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv-xI56KKOhU"
      },
      "source": [
        "# Read in the Equity Evaluation Corpus. Edit the file path below to go to the\n",
        "# file containing the Equity Evaluation Corpus.\n",
        "eec_sentences = dict()\n",
        "with open('/content/drive/MyDrive/NLP Capstone/data/Equity-Evaluation-Corpus.csv', 'r') as eec_file:\n",
        "    csv_reader = csv.reader(eec_file)\n",
        "    column_names = next(csv_reader)\n",
        "    for row in csv_reader:\n",
        "        if len(row[6]) == 0 and len(row[7]) == 0:\n",
        "            continue\n",
        "        cur_key = (row[2], row[7])\n",
        "        if cur_key not in eec_sentences:\n",
        "            eec_sentences[cur_key] = {\n",
        "                'male-name': [],\n",
        "                'female-name': [],\n",
        "                'male-np': [],\n",
        "                'female-np': []\n",
        "            }\n",
        "        if row[4] == 'male':\n",
        "            if len(row[5]) == 0:\n",
        "                eec_sentences[cur_key]['male-np'].append(row[1])\n",
        "            else:\n",
        "                eec_sentences[cur_key]['male-name'].append(row[1])\n",
        "        else:\n",
        "            if len(row[5]) == 0:\n",
        "                eec_sentences[cur_key]['female-np'].append(row[1])\n",
        "            else:\n",
        "                eec_sentences[cur_key]['female-name'].append(row[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INWPR7cnKR38"
      },
      "source": [
        "def model_bias_with_eec(model, tokenizer, eec_sentences, sig_level=0.05):\n",
        "    \"\"\"Measures gender bias in the model by comparing the differences in\n",
        "    sentiment scores when using male vs. female names or noun phrases for\n",
        "    each template sentence and emotion word in the Equity Evaluation Corpus.\n",
        "\n",
        "    Args:\n",
        "        model: An instance of PyTorch torch.nn.Module.\n",
        "        tokenizer: PreTrainedTokenizer.\n",
        "        eec_sentences: Dictionary where the key is (template sentence,\n",
        "        emotion word) and the value is a dictionary where the keys are\n",
        "        'male-name', 'female-name', 'male-np', and 'female-np', and the\n",
        "        value for each of these keys is a list of sentences from the Equity\n",
        "        Evaluation Corpus.\n",
        "        sig_level: Significance threshold used for a t-test.\n",
        "    \n",
        "    Returns:\n",
        "        The gender bias in the model based on the sentiment scores for the\n",
        "        sentences in the Equity Evaluation Corpus.\n",
        "    \"\"\"\n",
        "    sig_vals = []\n",
        "    not_sig_vals = []\n",
        "    for template, emotion in eec_sentences:\n",
        "        cur_key = (template, emotion)\n",
        "        male_names_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['male-name'])\n",
        "        avg_male_name_output = torch.mean(male_names_output[:, 0]).item()\n",
        "        female_names_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['female-name'])\n",
        "        avg_female_name_output = torch.mean(female_names_output[:, 0]).item()\n",
        "        male_nps_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['male-np'])\n",
        "        female_nps_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['female-np'])\n",
        "        all_male_output = [avg_male_name_output] + [output[0].item() for output in male_nps_output]\n",
        "        all_female_output = [avg_female_name_output] + [output[0].item() for output in female_nps_output]\n",
        "        p_val = ttest_rel(all_male_output, all_female_output)[1]\n",
        "        if p_val < sig_level:\n",
        "            not_sig_vals.append((template, emotion, p_val))\n",
        "        else:\n",
        "            sig_vals.append((template, emotion, p_val))\n",
        "    return (sig_vals, not_sig_vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Mxc02ViL8T"
      },
      "source": [
        "## (epsilon-k)-Fairness\n",
        "Metric defined in [Ma et al. (2020)](https://www.semanticscholar.org/paper/Metamorphic-Testing-and-Certified-Mitigation-of-in-Ma-Wang/5f5e9366983b53d4a753627d1144daa8e890e02f?p2df)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U40e-zQizwx"
      },
      "source": [
        "def epsilon_k_fairness(model, tokenizer, input, epsilon, k):\n",
        "    \"\"\"Measures epsilon-k fairness given an epsilon and k value.\n",
        "\n",
        "    Args:\n",
        "        model: An instance of PyTorch torch.nn.Module.\n",
        "        tokenizer: PreTrainedTokenizer.\n",
        "        input: the input sentence\n",
        "        epsilon: flexibility to degree of fairness\n",
        "        k: amount of mutations to compare to\n",
        "    \n",
        "    Returns:\n",
        "        The fairness measured as the difference between the real measured output\n",
        "        smoothed epsilon-k output\n",
        "    \"\"\"\n",
        "    real_predicted = get_model_output(model, tokenizer, input)\n",
        "    smoothed_output = real_predicted * (math.exp(epsilon) / (k + math.exp(epsilon)))\n",
        "    mutations = get_mutations(k)\n",
        "    for mutation in mutations:\n",
        "        mutation_output = get_model_output(model, tokenizer, mutation)\n",
        "        smoothed_output += (1 / (k + math.exp(epsilon)))\n",
        "    fairness = smoothed_output - real_predicted\n",
        "    return fairness"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYdKtxdn3689"
      },
      "source": [
        "def get_mutations(num):\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}