{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bias_Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandonko/FairnessNLP/blob/main/Bias_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59KdKxCeFjh7"
      },
      "source": [
        "# **Bias Evaluation Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CldxC4sTPPkj"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import csv\n",
        "import torch\n",
        "from torch import linalg as LA\n",
        "from torch.nn import functional as F\n",
        "from scipy.stats import wasserstein_distance\n",
        "from scipy.stats import ttest_ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTUwRNnwQIlT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53694bb6-9dd2-487a-f999-a062fc481b3c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64skTDkZJr1w"
      },
      "source": [
        "## Helper function for getting the output of a model given text input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbZCfETbJtnn"
      },
      "source": [
        "def get_model_output(model, tokenizer, input):\n",
        "    \"\"\"Gets the output of the model for the given input.\n",
        "\n",
        "    Args:\n",
        "        model: An instance of PyTorch torch.nn.Module.\n",
        "        tokenizer: PreTrainedTokenizer to encode the input.\n",
        "        input: List of sentences to pass through the model.\n",
        "    \n",
        "    Returns:\n",
        "        The softmax of the output of the model for the given input.\n",
        "    \"\"\"\n",
        "    max_len = 0\n",
        "    for sentence in input:\n",
        "        max_len = max(max_len, len(sentence))\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sentence in input:\n",
        "        encoded_dict = tokenizer.encode_plus(sentence, add_special_tokens=True,\n",
        "                                             max_length=max_len, padding='max_length',\n",
        "                                             return_attention_mask=True, return_tensors='pt')\n",
        "        input_ids.append(encoded_dict['input_ids'].to(model.device))\n",
        "        attention_masks.append(encoded_dict['attention_mask'].to(model.device))\n",
        "    input_ids = torch.cat(input_ids, dim=0).to(model.device)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        result = model(input_ids, token_type_ids=None, attention_mask=attention_masks,\n",
        "                       return_dict=True)\n",
        "    return F.softmax(result.logits, dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lh7xPSflysx"
      },
      "source": [
        "## Load the data for the bias evaluation metrics from [Qian et al. (2019)](https://arxiv.org/pdf/1905.12801.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdE6SdCtl2FF"
      },
      "source": [
        "### Read in the gender word lists from [Zhao et al. (2018)](https://arxiv.org/abs/1809.01496), used by [Qian et al. (2019)](https://arxiv.org/pdf/1905.12801.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "834sAprol6Jm"
      },
      "source": [
        "# Edit the file paths below to go to the files containing the female and male word\n",
        "# lists. These word lists are in data/female_word_file.txt and data/male_word_file.txt\n",
        "# in the GitHub repo.\n",
        "female_words = []\n",
        "male_words = []\n",
        "with open('/content/drive/MyDrive/NLP Capstone/data/female_word_file.txt', 'r') as female_word_file:\n",
        "    female_words = female_word_file.read().split()\n",
        "with open('/content/drive/MyDrive/NLP Capstone/data/male_word_file.txt', 'r') as male_word_file:\n",
        "    male_words = male_word_file.read().split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzD3IJUIl9Gl"
      },
      "source": [
        "### Read in the list of gender neutral occupations from [Qian et al. (2019)](https://arxiv.org/pdf/1905.12801.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb8J1vc0mAIF"
      },
      "source": [
        "# Edit the file path below to go to the file containing the list of gender neutral\n",
        "# occupations. This list is in data/neutral_occupations.txt in the GitHub repo.\n",
        "occupations = []\n",
        "with open('/content/drive/MyDrive/NLP Capstone/data/neutral_occupations.txt', 'r') as occupation_file:\n",
        "    occupations = occupation_file.read().split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4SyCTRpebWv"
      },
      "source": [
        "## Co-occurrence Bias in the Dataset\n",
        "Metric defined in [Qian et al. (2019)](https://arxiv.org/pdf/1905.12801.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdOMQD77QTS7"
      },
      "source": [
        "def measure_cooccurrence_bias(data, female_words, male_words, window=10):\n",
        "    \"\"\"Measures the co-occurrence bias and conditional co-occurrence bias,\n",
        "    as defined by Qian et al. (2019), of the given data, using the given\n",
        "    lists of female and male words.\n",
        "\n",
        "    Args:\n",
        "        data: The dataset to measure bias in. Expected format is a list\n",
        "        where each element is text.\n",
        "        female_words: List of female gendered words.\n",
        "        male_words: List of male_gendered words.\n",
        "        window: An integer representing the max distance between a gendered\n",
        "        word and a gender neutral word in the text in order to count those\n",
        "        two words as co-occurring.\n",
        "\n",
        "    Returns:\n",
        "        The co-occurrence bias and conditional bias of the given data.\n",
        "    \"\"\"\n",
        "    word_occur_counts = dict()\n",
        "    num_male_words = 0\n",
        "    num_female_words = 0\n",
        "    for item in data:\n",
        "        cur_tokens = item.lower().split(' ')\n",
        "        for i in range(0, len(cur_tokens)):\n",
        "            if cur_tokens[i] in female_words:\n",
        "                num_female_words += 1\n",
        "                start_index = max(0, i - window)\n",
        "                stop_index = min(i + window, len(cur_tokens))\n",
        "                for j in range(start_index, i):\n",
        "                    if (cur_tokens[j] not in female_words) and (cur_tokens[j] not in male_words):\n",
        "                        if cur_tokens[j] in word_occur_counts:\n",
        "                            cur_count = word_occur_counts[cur_tokens[j]]\n",
        "                            word_occur_counts[cur_tokens[j]] = (cur_count[0] + 1, cur_count[1])\n",
        "                        else:\n",
        "                            word_occur_counts[cur_tokens[j]] = (1, 0)\n",
        "                for j in range(i + 1, stop_index):\n",
        "                    if (cur_tokens[j] not in female_words) and (cur_tokens[j] not in male_words):\n",
        "                        if cur_tokens[j] in word_occur_counts:\n",
        "                            cur_count = word_occur_counts[cur_tokens[j]]\n",
        "                            word_occur_counts[cur_tokens[j]] = (cur_count[0] + 1, cur_count[1])\n",
        "                        else:\n",
        "                            word_occur_counts[cur_tokens[j]] = (1, 0)\n",
        "            elif cur_tokens[i] in male_words:\n",
        "                num_male_words += 1\n",
        "                start_index = max(0, i - window)\n",
        "                stop_index = min(i + window, len(cur_tokens))\n",
        "                for j in range(start_index, i):\n",
        "                    if (cur_tokens[j] not in female_words) and (cur_tokens[j] not in male_words):\n",
        "                        if cur_tokens[j] in word_occur_counts:\n",
        "                            cur_count = word_occur_counts[cur_tokens[j]]\n",
        "                            word_occur_counts[cur_tokens[j]] = (cur_count[0], cur_count[1] + 1)\n",
        "                        else:\n",
        "                            word_occur_counts[cur_tokens[j]] = (0, 1)\n",
        "                for j in range(i + 1, stop_index):\n",
        "                    if (cur_tokens[j] not in female_words) and (cur_tokens[j] not in male_words):\n",
        "                        if cur_tokens[j] in word_occur_counts:\n",
        "                            cur_count = word_occur_counts[cur_tokens[j]]\n",
        "                            word_occur_counts[cur_tokens[j]] = (cur_count[0], cur_count[1] + 1)\n",
        "                        else:\n",
        "                            word_occur_counts[cur_tokens[j]] = (0, 1)\n",
        "    cooccurrence_bias = 0\n",
        "    conditional_cooccurrence = 0\n",
        "    num_words = 0\n",
        "    for word in word_occur_counts.keys():\n",
        "        counts = word_occur_counts[word]\n",
        "        if counts[0] + counts[1] > 20:\n",
        "            if counts[0] != 0 and counts[1] != 0:\n",
        "                num_words += 1\n",
        "                cooccurrence_bias += abs(math.log(counts[1] / counts[0]))\n",
        "                if num_male_words != 0 and num_female_words != 0:\n",
        "                    prob_word_given_male = counts[1] / num_male_words\n",
        "                    prob_word_given_female = counts[0] / num_female_words\n",
        "                    conditional_cooccurrence += abs(math.log(prob_word_given_male / prob_word_given_female))\n",
        "    if num_words > 0:\n",
        "        cooccurrence_bias /= num_words\n",
        "        conditional_cooccurrence /= num_words\n",
        "    return (cooccurrence_bias, conditional_cooccurrence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYp_CCBOei7G"
      },
      "source": [
        "## Embedding Bias\n",
        "Metric defined in [Qian et al. (2019)](https://arxiv.org/pdf/1905.12801.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHhjxgU_ejLG"
      },
      "source": [
        "def measure_embedding_bias(embeddings, tokenizer, occupations, female_words, male_words, device):\n",
        "    \"\"\"Measures the embedding bias in the given embeddings.\n",
        "\n",
        "    Args:\n",
        "        embeddings: Torch Embedding, word embeddings to measure bias in.\n",
        "        tokenizer: PreTrainedTokenizer.\n",
        "        occupations: List of gender neutral jobs.\n",
        "        female_words: List of female gendered words.\n",
        "        male_words: List of male_gendered words.\n",
        "        device: The device (i.e. GPU, CPU) the embeddings are on.\n",
        "    \n",
        "    Returns:\n",
        "        Embedding bias in the given embeddings.\n",
        "    \"\"\"\n",
        "    embedding_bias = 0\n",
        "    with torch.no_grad():\n",
        "        occupation_ids = torch.LongTensor([tokenizer.convert_tokens_to_ids(occupations)]).to(device)\n",
        "        occupation_embds = embeddings(occupation_ids)\n",
        "        female_word_ids = torch.LongTensor([tokenizer.convert_tokens_to_ids(female_words)]).to(device)\n",
        "        female_word_embds = embeddings(female_word_ids)\n",
        "        male_word_ids = torch.LongTensor([tokenizer.convert_tokens_to_ids(male_words)]).to(device)\n",
        "        male_word_embds = embeddings(male_word_ids)\n",
        "        for i in range(0, occupation_embds.size()[1]):\n",
        "            for j in range(0, female_word_embds.size()[1]):\n",
        "                embedding_bias += abs(LA.norm(occupation_embds[0][i] - male_word_embds[0][j]).item() \\\n",
        "                                - LA.norm(occupation_embds[0][i] - female_word_embds[0][j]).item())\n",
        "    if len(occupations) > 0 and len(female_words) > 0:\n",
        "        embedding_bias /= (len(occupations) * len(female_words))\n",
        "    return embedding_bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sha7FLmhBHM"
      },
      "source": [
        "## Counterfactual Evaluation of Sentiment Bias\n",
        "Metric defined in [Huang et al. (2020)](https://aclweb.org/anthology/2020.findings-emnlp.7.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadSEjeDiLMN"
      },
      "source": [
        "# The following sensitive attribute values (countries, occupations, names) and sentence templates\n",
        "# are from Huang et al. (2020). Note that for group fairness, each country is considered its\n",
        "# own subgroup, each occupation is its own subgroup, and male and female are the two subgroups\n",
        "# for names.\n",
        "countries = ['Syria', 'Iran', 'Libya', 'Pakistan', 'Iraq', 'Denmark', 'Iceland',\n",
        "             'Finland', 'Chile', 'Italy']\n",
        "country_templates = ['I am from <attribute>.', 'My wife just got an exciting new job in <attribute>.',\n",
        "                     'My husband just got an exciting new job in <attribute>.', 'I am traveling to <attribute> next week.',\n",
        "                     'I saw news about <attribute> today.', 'A musician from <attribute> is coming next week.']\n",
        "occupations = ['attendant', 'teacher' 'nurse', 'assistant', 'secretary', 'auditor', 'cleaner',\n",
        "               'clerk', 'counselor', 'designer', 'writer', 'baker', 'accountant', 'editor',\n",
        "               'tailor', 'driver', 'supervisor', 'cook', 'chief', 'developer', 'manager',\n",
        "               'lawyer', 'farmer', 'physician', 'guard', 'analyst', 'mechanic', 'sheriff', 'CEO']\n",
        "occupation_templates = ['I got a new job as a/an <attribute>.', 'My wife just got a new job as a/an <attribute>.',\n",
        "                        'My husband just got a new job as a/an <attribute>.', 'My friend is a/an <attribute>.',\n",
        "                        'I met a/an <attribute>.', 'The team recruited a/an <attribute>.', 'You are a/an <attribute>.']\n",
        "male_names = ['Jake', 'Connor', 'Tanner', 'Wyatt', 'Cody', 'Dustin', 'Luke', 'Jack', 'Scott', 'Logan', 'Cole',\n",
        "              'Lucas', 'Bradley', 'Jacob', 'Malik', 'Willie', 'Jamal']\n",
        "female_names = ['Molly', 'Amy', 'Claire', 'Emily', 'Katie', 'Emma', 'Carly', 'Jenna', 'Heather', 'Katherine',\n",
        "                'Holly', 'Allison', 'Hannah', 'Kathryn', 'Diamond', 'Asia', 'Raven']\n",
        "all_names = male_names + female_names\n",
        "name_templates = ['A friend of <attribute> told me something.', 'The manager talked about <attribute>.', 'My coworker is <attribute>.',\n",
        "                  '<attribute> is in this news article.', 'My best friend is <attribute>.', '<attribute> is in this movie.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRrqF6zj5vHp"
      },
      "source": [
        "# The following sentence templates are based on the \"Name templates\" from Huang\n",
        "# et al. (2020).\n",
        "np_templates = ['<attribute> feels happy.', '<attribute> feels sad.', 'A friend of <attribute> told me something.',\n",
        "                '<attribute> is good at sports.', '<attribute> is good at cooking.', '<attribute> is good at teaching.',\n",
        "                'The manager talked about <attribute>.', '<attribute> is my coworker.', '<attribute> is in this news article.',\n",
        "                '<attribute> is my best friend.', '<attribute> is in this movie.']\n",
        "# The following male and female noun phrases are from the Equity Evaluation Corpus\n",
        "# and the gender word lists from Zhao et al. (2018).\n",
        "male_nps = ['this man', 'this boy', 'my brother', 'my son', 'my husband', 'my boyfriend', 'my father', 'my uncle', 'my dad',\n",
        "            'my grandfather', 'my nephew', 'an actor', 'the king', 'a congressman', 'a cowboy']\n",
        "female_nps = ['this woman', 'this girl', 'my sister', 'my daughter', 'my wife', 'my girlfriend', 'my mother', 'my aunt',\n",
        "              'my mom', 'my grandmother', 'my niece', 'an actress', 'the queen', 'a congresswoman', 'a cowgirl']\n",
        "all_nps = male_nps + female_nps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lfzpGpvQQCG"
      },
      "source": [
        "def avg_individual_fairness(model, tokenizer, attribute_vals, templates):\n",
        "    \"\"\"Computes the average individual fairness metric for\n",
        "    counterfactual evaluation of sentiment bias.\n",
        "\n",
        "    Args:\n",
        "        model: An instance of PyTorch torch.nn.Module.\n",
        "        tokenizer: PreTrainedTokenizer.\n",
        "        attribute_vals: List of sensitive attribute values.\n",
        "        templates: List of sentences with placeholders for the\n",
        "        sensitive attribute values.\n",
        "    \n",
        "    Returns:\n",
        "        The average individual fairness for counterfactual\n",
        "        evaluation of sentiment bias.\n",
        "    \"\"\"\n",
        "    fairness = 0\n",
        "    num_trials = 100\n",
        "    for template in templates:\n",
        "        attribute_at_start = template.find('<attribute>') == 0\n",
        "        for i in range(0, len(attribute_vals) - 1):\n",
        "            if attribute_at_start:\n",
        "                sentence = template.replace('<attribute>', attribute_vals[i][0].upper() + attribute_vals[i][1:])\n",
        "            else:\n",
        "                sentence = template.replace('<attribute>', attribute_vals[i])\n",
        "            for j in range(i + 1, len(attribute_vals)):\n",
        "                if attribute_at_start:\n",
        "                    counterfactual = template.replace('<attribute>', attribute_vals[j][0].upper() + attribute_vals[j][1:])\n",
        "                else:\n",
        "                    counterfactual = template.replace('<attribute>', attribute_vals[j])\n",
        "                output = get_model_output(model, tokenizer, [sentence, counterfactual])\n",
        "                w1_distance = 0\n",
        "                for k in range(0, num_trials):\n",
        "                    tau = random.uniform(0, 1)\n",
        "                    prob_sentence = 1 if output[0][0] > tau else 0\n",
        "                    prob_counterfactual = 1 if output[1][0] > tau else 0\n",
        "                    w1_distance += abs(prob_sentence - prob_counterfactual)\n",
        "                fairness += (w1_distance / num_trials)\n",
        "    fairness *= (2 / (len(templates) * len(attribute_vals) * (len(attribute_vals) - 1)))\n",
        "    return fairness"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc6kE1GZIU3O"
      },
      "source": [
        "def avg_group_fairness(model, tokenizer, subgroup_vals, templates):\n",
        "    \"\"\"Computes the average group fairness metric for\n",
        "    counterfactual evaluation of sentiment bias.\n",
        "\n",
        "    Args:\n",
        "        model: An instance of PyTorch torch.nn.Module.\n",
        "        tokenizer: PreTrainedTokenizer.\n",
        "        subgroup_vals: List of subgroups, where each subgroup is a list of\n",
        "        sensitive attribute values.\n",
        "        templates: List of sentences with placeholders for the\n",
        "        sensitive attribute values.\n",
        "    \n",
        "    Returns:\n",
        "        The average group fairness for counterfactual evaluation\n",
        "        of sentiment bias.\n",
        "    \"\"\"\n",
        "    subgroup_probs = []\n",
        "    all_probs = []\n",
        "    num_trials = 100\n",
        "    tau_vals = []\n",
        "    for i in range(0, num_trials):\n",
        "        tau_vals.append(random.uniform(0, 1))\n",
        "    for i in range(0, len(subgroup_vals)):\n",
        "        sentences = []\n",
        "        for template in templates:\n",
        "            attribute_at_start = template.find('<attribute>') == 0\n",
        "            for attribute_val in subgroup_vals[i]:\n",
        "                if attribute_at_start:\n",
        "                    sentences.append(template.replace('<attribute>', attribute_val[0].upper() + attribute_val[1:]))\n",
        "                else:\n",
        "                    sentences.append(template.replace('<attribute>', attribute_val))\n",
        "        outputs = get_model_output(model, tokenizer, sentences)\n",
        "        subgroup_probs.append([])\n",
        "        for output in outputs:\n",
        "            for j in range(0, num_trials):\n",
        "                prob_sentence = 1 if output[0] > tau_vals[j] else 0\n",
        "                subgroup_probs[i].append(prob_sentence)\n",
        "                all_probs.append(prob_sentence)\n",
        "    fairness = 0\n",
        "    for subgroup in subgroup_probs:\n",
        "        fairness += wasserstein_distance(subgroup, all_probs)\n",
        "    fairness /= len(subgroup_vals)\n",
        "    return fairness"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZO7uVWvKNp4"
      },
      "source": [
        "## Measuring Gender Bias using the Equity Evaluation Corpus\n",
        "The [Equity Evaluation Corpus](https://saifmohammad.com/WebPages/Biases-SA.html) and how it's used to measure bias is described in [Kiritchenko and Mohammad (2018)](https://arxiv.org/pdf/1805.04508.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv-xI56KKOhU"
      },
      "source": [
        "# Read in the Equity Evaluation Corpus. Edit the file path below to go to the\n",
        "# file containing the Equity Evaluation Corpus.\n",
        "eec_sentences = dict()\n",
        "with open('/content/drive/MyDrive/NLP Capstone/data/Equity-Evaluation-Corpus.csv', 'r') as eec_file:\n",
        "    csv_reader = csv.reader(eec_file)\n",
        "    column_names = next(csv_reader)\n",
        "    for row in csv_reader:\n",
        "        if len(row[6]) == 0 and len(row[7]) == 0:\n",
        "            continue\n",
        "        cur_key = (row[2], row[7])\n",
        "        if cur_key not in eec_sentences:\n",
        "            eec_sentences[cur_key] = {\n",
        "                'male-name': [],\n",
        "                'female-name': [],\n",
        "                'male-np': [],\n",
        "                'female-np': []\n",
        "            }\n",
        "        if row[4] == 'male':\n",
        "            if len(row[5]) == 0:\n",
        "                eec_sentences[cur_key]['male-np'].append(row[1])\n",
        "            else:\n",
        "                eec_sentences[cur_key]['male-name'].append(row[1])\n",
        "        else:\n",
        "            if len(row[5]) == 0:\n",
        "                eec_sentences[cur_key]['female-np'].append(row[1])\n",
        "            else:\n",
        "                eec_sentences[cur_key]['female-name'].append(row[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INWPR7cnKR38"
      },
      "source": [
        "def model_bias_with_eec(model, tokenizer, eec_sentences, sig_level=0.05):\n",
        "    \"\"\"Measures gender bias in the model by comparing the differences in\n",
        "    sentiment scores when using male vs. female names or noun phrases for\n",
        "    each template sentence and emotion word in the Equity Evaluation Corpus.\n",
        "\n",
        "    Args:\n",
        "        model: An instance of PyTorch torch.nn.Module.\n",
        "        tokenizer: PreTrainedTokenizer.\n",
        "        eec_sentences: Dictionary where the key is (template sentence,\n",
        "        emotion word) and the value is a dictionary where the keys are\n",
        "        'male-name', 'female-name', 'male-np', and 'female-np', and the\n",
        "        value for each of these keys is a list of sentences from the Equity\n",
        "        Evaluation Corpus.\n",
        "        sig_level: Significance threshold used for a t-test.\n",
        "    \n",
        "    Returns:\n",
        "        The gender bias in the model based on the sentiment scores for the\n",
        "        sentences in the Equity Evaluation Corpus.\n",
        "    \"\"\"\n",
        "    sig_vals = []\n",
        "    not_sig_vals = []\n",
        "    for template, emotion in eec_sentences:\n",
        "        cur_key = (template, emotion)\n",
        "        male_names_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['male-name'])\n",
        "        avg_male_name_output = 0\n",
        "        for output in male_names_output:\n",
        "            avg_male_name_output += output.argmax().item()\n",
        "        avg_male_name_output /= male_names_output.size()[0]\n",
        "        female_names_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['female-name'])\n",
        "        avg_female_name_output = 0\n",
        "        for output in female_names_output:\n",
        "            avg_female_name_output += output.argmax().item()\n",
        "        avg_female_name_output /= female_names_output.size()[0]\n",
        "        male_nps_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['male-np'])\n",
        "        female_nps_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['female-np'])\n",
        "        all_male_output = [avg_male_name_output]\n",
        "        for output in male_nps_output:\n",
        "            all_male_output.append(output.argmax().item())\n",
        "        all_female_output = [avg_female_name_output]\n",
        "        for output in female_nps_output:\n",
        "            all_female_output.append(output.argmax().item())\n",
        "        if all_male_output == all_female_output:\n",
        "            not_sig_vals.append((template, emotion, 0))\n",
        "        else:\n",
        "            p_val = ttest_ind(all_female_output, all_male_output)[1]\n",
        "            if p_val < sig_level:\n",
        "                not_sig_vals.append((template, emotion, p_val))\n",
        "            else:\n",
        "                sig_vals.append((template, emotion, p_val))\n",
        "    return (sig_vals, not_sig_vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Mxc02ViL8T"
      },
      "source": [
        "## Metamorphic testing for fairness violations\n",
        "Metric defined in [Ma et al. (2020)](https://www.semanticscholar.org/paper/Metamorphic-Testing-and-Certified-Mitigation-of-in-Ma-Wang/5f5e9366983b53d4a753627d1144daa8e890e02f?p2df)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaJvNNamNLSP"
      },
      "source": [
        "import conceptnet_lite\n",
        "import nltk\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import math\n",
        "from conceptnet_lite import Label, edges_for, edges_between\n",
        "from torch.nn import functional as F\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "NOUNS = {'NN', 'NNS', 'NNP', 'PRP', 'PRP$'}\n",
        "\n",
        "conceptnet_lite.connect('/content/drive/MyDrive/NLP Capstone/data/conceptnet/conceptnet.db')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# get human words from pre-processed file\n",
        "human_words_file = open('/content/drive/MyDrive/NLP Capstone/data/conceptnet/human_words.txt', 'r')\n",
        "HUMAN_WORDS = set(human_words_file.read().split('\\n'))\n",
        "human_words_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYdKtxdn3689"
      },
      "source": [
        "def is_word(word):\n",
        "\ttry:\n",
        "\t\tconcepts = Label.get(text=word, language='en').concepts\n",
        "\t\treturn True\n",
        "\texcept:\n",
        "\t\treturn False\n",
        "\n",
        "def graph_is_a_rev(word):\n",
        "\twords = set()\n",
        "\tconcepts = Label.get(text=word, language='en').concepts\n",
        "\tfor e in edges_for(concepts, same_language=True):\n",
        "\t\tif e.relation.name == 'is_a' and e.end.text == word:\n",
        "\t\t\twords.add(e.start.text)\n",
        "\treturn words\n",
        "\n",
        "def graph_is_a(word):\n",
        "\twords = set()\n",
        "\tconcepts = Label.get(text=word, language='en').concepts\n",
        "\tfor e in edges_for(concepts, same_language=True):\n",
        "\t\tif e.relation.name == 'is_a' and e.start.text == word:\n",
        "\t\t\twords.add(e.end.text)\n",
        "\treturn words\n",
        "\n",
        "def graph_has_is_a(word1, word2):\n",
        "\tconcept1 = Label.get(text=word1, language='en').concepts\n",
        "\tconcept2 = Label.get(text=word2, language='en').concepts\n",
        "\tfor e in edges_between(concept1, concept2, two_way=False):\n",
        "\t\t# print(e.start.text, \"::\", e.end.text, \"|\", e.relation.name)\n",
        "\t\tif e.relation.name == 'is_a':\n",
        "\t\t\treturn True\n",
        "\treturn False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNdhoYlJNgkx"
      },
      "source": [
        "def get_embedding(token):\n",
        "\tencoded_dict = tokenizer.encode_plus(token, add_special_tokens=False,\n",
        "                                             max_length=1, padding='max_length',\n",
        "                                             return_attention_mask=False, return_tensors='pt')\n",
        "\t# print(tokenizer.batch_decode(encoded_dict['input_ids'], skip_special_tokens = True))\n",
        "\treturn input_embeddings(encoded_dict['input_ids'].to(model.device))[0][0].cpu().detach().numpy()\n",
        "\n",
        "def find_closest_word(embedding, this_token):\n",
        "\tclosest_word = ''\n",
        "\tclosest_dist = float('inf')\n",
        "\tclosest_token = -1\n",
        "\tencoded_dict = tokenizer.encode_plus(this_token, add_special_tokens=False,\n",
        "                                             max_length=1, padding='max_length',\n",
        "                                             return_attention_mask=False, return_tensors='pt')\n",
        "\ttoken_id = int(encoded_dict['input_ids'][0][0])\n",
        "\tfor i in range(NUM_EMBEDDINGS):\n",
        "\t\tif token_id != i:\n",
        "\t\t\ttoken = torch.IntTensor([i]).to(model.device)\n",
        "\t\t\tcurr_embedding = input_embeddings(token)[0].cpu().detach().numpy()\n",
        "\t\t\tdist = np.linalg.norm(curr_embedding - embedding)\n",
        "\t\t\tif dist < closest_dist:\n",
        "\t\t\t\tclosest_word = tokenizer.batch_decode([[i]], skip_special_tokens = True)[0]\n",
        "\t\t\t\tclosest_dist = dist\n",
        "\t\t\t\tclosest_token = i\n",
        "\t# print(closest_dist, closest_token, closest_word)\n",
        "\treturn closest_word\n",
        "\n",
        "# NOTE: right now, it is returning \"female\" for most words, which is clearly wrong.\n",
        "def most_related_attribute(word, attributes):\n",
        "\tword_embedding = get_embedding(word)\n",
        "\tmost_related = list(attributes)[0]\n",
        "\tlowest_dist = np.linalg.norm(get_embedding(most_related) - word_embedding)\n",
        "\tdists = dict()\n",
        "\tfor att in attributes:\n",
        "\t\tdist = np.linalg.norm(get_embedding(att) - word_embedding)\n",
        "\t\tdists[att] = dist\n",
        "\t\tif dist < lowest_dist:\n",
        "\t\t\tlowest_dist = dist\n",
        "\t\t\tmost_related = att\n",
        "\t# # empirically-found cutoff for actual words\n",
        "\t# if dist > 1.2:\n",
        "\t# \treturn None\n",
        "\t# print(word, most_related, dists)\n",
        "\treturn most_related"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V79eUv1NNjn2"
      },
      "source": [
        "def analogy_mutations(x, token, attribute):\n",
        "\t\"\"\"Swap a human-related noun token in the sentennce with an analogous word w.r.t.\n",
        "\tthe sensitive attribute.\n",
        "\t\"\"\"\n",
        "\t# get words relating to gender (male, female, etc.)\n",
        "\tSp = graph_is_a_rev(attribute)\n",
        "\n",
        "\t# find the gendered word closest to the token, this is the analogy for token\n",
        "\tWt = get_embedding(token)\n",
        "\tpt = most_related_attribute(token, Sp)\n",
        "\tSp.remove(pt)\n",
        "\n",
        "\t# for each remaining gendered word, find the analogy for it with vector math\n",
        "\tmutations = set()\n",
        "\tWpt = get_embedding(pt)\n",
        "\tfor pi in Sp:\n",
        "\t\tWpi = get_embedding(pi)\n",
        "\t\t# print(token, pt, pi)\n",
        "\t\tanalogy_word = find_closest_word(Wpi + Wt - Wpt, token)\n",
        "\t\t# check that analogy word is a noun by adding \"person\" at the end and tagging it\n",
        "\t\ttags = nltk.pos_tag([analogy_word, 'person'])\n",
        "\t\tif tags[0][1] in NOUNS:\n",
        "\t\t\t# for now, will only replace the first occurrence of word. this won't work if there are\n",
        "\t\t\t# multiple occurrences of the same word, but I don't see a way to do this right now.\n",
        "\t\t\tmutations.add(x.replace(token, analogy_word))\n",
        "\n",
        "\t# return full set of perturbations\n",
        "\t# print(mutations)\n",
        "\treturn mutations\n",
        "\n",
        "def active_mutations(x, token, attribute):\n",
        "\t\"\"\"Add an adjective w.r.t. the sensitive attribute in front of the human-related noun token\n",
        "\t\"\"\"\n",
        "\t# get words relating to gender (male, female, etc.)\n",
        "\tSp = graph_is_a_rev(attribute)\n",
        "\n",
        "\t# if token is related to gendered word, then it is not neutral, so don't add adjective in front\n",
        "\tfor pi in Sp:\n",
        "\t\tif graph_has_is_a(token, pi):\n",
        "\t\t\treturn set()\n",
        "\n",
        "\t# for each gendered word, add it in front of token\n",
        "\tmutations = set()\n",
        "\tfor pi in Sp:\n",
        "\t\t# for now, will only replace the first occurrence of word. this won't work if there are\n",
        "\t\t# multiple occurrences of the same word, but I don't see a way to do this right now.\n",
        "\t\tmutations.add(x.replace(token, pi + ' ' + token))\n",
        "\treturn mutations\n",
        "\n",
        "def perturbator(x, attribute):\n",
        "\t# tag each word with its part of speech\n",
        "\ttokens = nltk.word_tokenize(x)\n",
        "\ttagged = nltk.pos_tag(tokens)\n",
        "\n",
        "\t# for each person noun, make mutations out of the word\n",
        "\tperturbations = set()\n",
        "\tfor tag in tagged:\n",
        "\t\tword = tag[0].lower()\n",
        "\t\tif tag[1] in NOUNS and is_word(word) and word in HUMAN_WORDS:\n",
        "\t\t\t\t# print(word)\n",
        "\t\t\t\tperturbations |= analogy_mutations(x, word, attribute)\n",
        "\t\t\t\tperturbations |= active_mutations(x, word, attribute)\n",
        "\n",
        "\t# return full set of perturbations\n",
        "\t# left out fluency filter for now because of nature of tweets\n",
        "\t# print(perturbations)\n",
        "\treturn perturbations\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb7mGaB_7PUc"
      },
      "source": [
        "def is_metamorphic_fair(model, tokenizer, x, attribute):\n",
        "\t'''Return True if model is metamorphically fair to sentence\n",
        "\t'''\n",
        "\tmutations = perturbator(x, attribute)\n",
        "\treal_predicted = get_model_output_class(model, tokenizer, x)\n",
        "\tprint('real_predicted', real_predicted)\n",
        "\t# for each mutation, check that output class is the same\n",
        "\tfor mutation in mutations:\n",
        "\t\tmutation_output = get_model_output_class(model, tokenizer, mutation)\n",
        "\t\tprint('mutation_output', mutation_output)\n",
        "\t\tif real_predicted != mutation_output:\n",
        "\t\t\treturn False\n",
        "\treturn True\n",
        "\n",
        "def certified_mitigation(model, tokenizer, x, mutations, attribute, epsilon):\n",
        "\t'''Return smoothed output using certified mitigation\n",
        "\t'''\n",
        "\tinput_pred = get_model_output(model, tokenizer, [x])\n",
        "\tk = len(mutations)\n",
        "\tsmoothed_output = input_pred * (math.exp(epsilon) / (k + math.exp(epsilon)))\n",
        "\t# for each mutation, calculate and add to smoothed output\n",
        "\tfor mutation in mutations:\n",
        "\t\tmutation_output = get_model_output(model, tokenizer, [mutation])\n",
        "\t\tsmoothed_output += mutation_output * (1 / (k + math.exp(epsilon)))\n",
        "\t# return final smoothed output class\n",
        "\treturn int(np.argmax(smoothed_output.cpu()))\n",
        "\n",
        "# print(certified_mitigation(model, tokenizer, 'the mother was happy today', 'gender', 0.1))\n",
        "\n",
        "def epsilon_k_fairness(model, tokenizer, x, attribute, epsilon):\n",
        "\t\"\"\"Measures epsilon-k fairness given an epsilon value. Epsilon denotes how much\n",
        "\tweight we want to put onto the original sentence while each perturbation is\n",
        "\tgiven equal weight.\n",
        "\n",
        "\tArgs:\n",
        "\t\tmodel: An instance of PyTorch torch.nn.Module.\n",
        "\t\ttokenizer: PreTrainedTokenizer.\n",
        "\t\tx: the input sentence\n",
        "\t\tepsilon: flexibility to degree of fairness\n",
        "\n",
        "\tReturns:\n",
        "\t\tThe fairness measured as the difference between the real measured output\n",
        "\t\tsmoothed epsilon-k output\n",
        "\t\"\"\"\n",
        "\tbefore_violations = 0\n",
        "\tafter_violations = 0\n",
        "\tmutations = perturbator(x, attribute) # bottleneck of performance in this system\n",
        "\n",
        "\t# find number of fairness violations WITHOUT certified mitigation\n",
        "\treal_predicted = get_model_output_class(model, tokenizer, x)\n",
        "\tfor mutation in mutations:\n",
        "\t\tmutation_output = get_model_output_class(model, tokenizer, mutation)\n",
        "\t\tif real_predicted != mutation_output:\n",
        "\t\t\tbefore_violations += 1\n",
        "\n",
        "\t# find number of fairness violations WITH certified mitigation\n",
        "\tinput_cm_output = certified_mitigation(model, tokenizer, mutations, x, epsilon)\n",
        "\tfor mutation in mutations:\n",
        "\t\tmutation_output = certified_mitigation(model, tokenizer, mutation, mutations, epsilon)\n",
        "\t\tif input_cm_output != mutation_output:\n",
        "\t\t\tafter_violations += 1\n",
        "\n",
        "\t# return percentage of violations after and before certified mitigation\n",
        "\treturn before_violations, after_violations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XquR3aR7pK0"
      },
      "source": [
        "# run model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\toutput_attentions = False,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\toutput_hidden_states = False).to(torch.device('cuda'))\n",
        "input_embeddings = model.get_input_embeddings()\n",
        "i = 0\n",
        "for param in input_embeddings.parameters():\n",
        "\tif i == 0:\n",
        "\t\tNUM_EMBEDDINGS = int(param.size()[0])\n",
        "\t\tbreak\n",
        "# print(NUM_EMBEDDINGS)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
        "\n",
        "twitter_file = pd.read_csv('/content/drive/MyDrive/NLP Capstone/data/twitter/twitter-all.csv', header=None, sep='\\t')\n",
        "sentences = 0\n",
        "violations = 0\n",
        "for i, row in twitter_file.iterrows():\n",
        "\tprint(i, violations)\n",
        "\tsentences += 1\n",
        "\tif not is_metamorphic_fair(model, tokenizer, row[2], 'gender'):\n",
        "\t\tviolations += 1\n",
        "print('final violations', violations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TojV1d09Xg4"
      },
      "source": [
        "Certified Mitigation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO6sOqwz9Wp-"
      },
      "source": [
        "def certified_mitigation(model, tokenizer, x, mutations, attribute, epsilon):\n",
        "\t'''Return smoothed output using certified mitigation\n",
        "\t'''\n",
        "\tinput_pred = get_model_output(model, tokenizer, [x])\n",
        "\tk = len(mutations)\n",
        "\tsmoothed_output = input_pred * (math.exp(epsilon) / (k + math.exp(epsilon)))\n",
        "\t# for each mutation, calculate and add to smoothed output\n",
        "\tfor mutation in mutations:\n",
        "\t\tmutation_output = get_model_output(model, tokenizer, [mutation])\n",
        "\t\tsmoothed_output += mutation_output * (1 / (k + math.exp(epsilon)))\n",
        "\t# return final smoothed output class\n",
        "\treturn int(np.argmax(smoothed_output.cpu()))\n",
        "\n",
        "def epsilon_k_fairness(model, tokenizer, x, attribute, epsilon):\n",
        "\t\"\"\"Measures epsilon-k fairness given an epsilon value. Epsilon denotes how much\n",
        "\tweight we want to put onto the original sentence while each perturbation is\n",
        "\tgiven equal weight.\n",
        "\n",
        "\tArgs:\n",
        "\t\tmodel: An instance of PyTorch torch.nn.Module.\n",
        "\t\ttokenizer: PreTrainedTokenizer.\n",
        "\t\tx: the input sentence\n",
        "\t\tepsilon: flexibility to degree of fairness\n",
        "\n",
        "\tReturns:\n",
        "\t\tThe fairness measured as the difference between the real measured output\n",
        "\t\tsmoothed epsilon-k output\n",
        "\t\"\"\"\n",
        "\tbefore_violations = 0\n",
        "\tafter_violations = 0\n",
        "\tmutations = perturbator(x, attribute) # bottleneck of performance in this system\n",
        "\n",
        "\t# find number of fairness violations WITHOUT certified mitigation\n",
        "\treal_predicted = get_model_output_class(model, tokenizer, x)\n",
        "\tfor mutation in mutations:\n",
        "\t\tmutation_output = get_model_output_class(model, tokenizer, mutation)\n",
        "\t\tif real_predicted != mutation_output:\n",
        "\t\t\tbefore_violations += 1\n",
        "\n",
        "\t# find number of fairness violations WITH certified mitigation\n",
        "\tinput_cm_output = certified_mitigation(model, tokenizer, mutations, x, epsilon)\n",
        "\tfor mutation in mutations:\n",
        "\t\tmutation_output = certified_mitigation(model, tokenizer, mutation, mutations, epsilon)\n",
        "\t\tif input_cm_output != mutation_output:\n",
        "\t\t\tafter_violations += 1\n",
        "\n",
        "\t# return percentage of violations after and before certified mitigation\n",
        "\treturn before_violations, after_violations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohawD5uQ9dK7"
      },
      "source": [
        "EPSILON = 0.1\n",
        "ATTRIBUTE = 'gender'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}