{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bias_Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandonko/FairnessNLP/blob/main/Bias_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59KdKxCeFjh7"
      },
      "source": [
        "# **Bias Evaluation Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CldxC4sTPPkj"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import csv\n",
        "import torch\n",
        "from torch import linalg as LA\n",
        "from torch.nn import functional as F\n",
        "from scipy.stats import wasserstein_distance\n",
        "from scipy.stats import ttest_ind\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTUwRNnwQIlT",
        "outputId": "e9668599-e4b7-43a3-b842-78cb965df10b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64skTDkZJr1w"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbZCfETbJtnn"
      },
      "source": [
        "def get_model_output(model, tokenizer, input):\n",
        "    \"\"\"Gets the output of the model for the given input.\n",
        "\n",
        "    Args:\n",
        "        model: An instance of PyTorch torch.nn.Module.\n",
        "        tokenizer: PreTrainedTokenizer to encode the input.\n",
        "        input: List of sentences to pass through the model.\n",
        "    \n",
        "    Returns:\n",
        "        The softmax of the output of the model for the given input.\n",
        "    \"\"\"\n",
        "    max_len = 0\n",
        "    for sentence in input:\n",
        "        max_len = max(max_len, len(sentence))\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sentence in input:\n",
        "        encoded_dict = tokenizer.encode_plus(sentence, add_special_tokens=True,\n",
        "                                             max_length=max_len, padding='max_length',\n",
        "                                             return_attention_mask=True, return_tensors='pt')\n",
        "        input_ids.append(encoded_dict['input_ids'].to(model.device))\n",
        "        attention_masks.append(encoded_dict['attention_mask'].to(model.device))\n",
        "    input_ids = torch.cat(input_ids, dim=0).to(model.device)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        result = model(input_ids, token_type_ids=None, attention_mask=attention_masks,\n",
        "                       return_dict=True)\n",
        "    return F.softmax(result.logits, dim=-1)\n",
        "\n",
        "def get_model_output_class(model, tokenizer, input):\n",
        "    return torch.argmax(get_model_output(model, tokenizer, input), dim = 1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMCw1jy7a-S7"
      },
      "source": [
        "def split_by_predictability(pred_scores):\n",
        "    \"\"\"Splits the data into easy, medium, and hard based on\n",
        "    predictability scores (how many times the model correctly\n",
        "    classified that datapoint).\n",
        "\n",
        "    Args:\n",
        "        pred_scores: List of predictability scores, where each\n",
        "        element is of the form (data, number of times that data\n",
        "        was correctly classified by the model).\n",
        "    \n",
        "    Returns:\n",
        "        A list of 3 lists, containing the easy, medium, and hard\n",
        "        data and their predictability scores.\n",
        "    \"\"\"\n",
        "    if len(pred_scores) == 0:\n",
        "        return [[], [], []]\n",
        "    max_pred_score = pred_scores[0][1]\n",
        "    min_pred_score = pred_scores[0][1]\n",
        "    for data_score in pred_scores:\n",
        "        max_pred_score = max(max_pred_score, data_score[1])\n",
        "        min_pred_score = min(min_pred_score, data_score[1])\n",
        "    if max_pred_score == min_pred_score:\n",
        "        return [[], pred_scores, []]\n",
        "    bucket_size = int((max_pred_score - min_pred_score + 1) / 3)\n",
        "    hard_cutoff = min_pred_score + bucket_size\n",
        "    easy_threshold = max_pred_score - bucket_size\n",
        "    difficulty_buckets = [[], [], []]\n",
        "    for data_score in pred_scores:\n",
        "        if data_score[1] > easy_threshold:\n",
        "            difficulty_buckets[0].append(data_score)\n",
        "        elif data_score[1] < hard_cutoff:\n",
        "            difficulty_buckets[2].append(data_score)\n",
        "        else:\n",
        "            difficulty_buckets[1].append(data_score)\n",
        "    return difficulty_buckets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lh7xPSflysx"
      },
      "source": [
        "## Load the data for the bias evaluation metrics from [Qian et al. (2019)](https://arxiv.org/pdf/1905.12801.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdE6SdCtl2FF"
      },
      "source": [
        "### Read in the gender word lists from [Zhao et al. (2018)](https://arxiv.org/abs/1809.01496), used by [Qian et al. (2019)](https://arxiv.org/pdf/1905.12801.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "834sAprol6Jm"
      },
      "source": [
        "# Edit the file paths below to go to the files containing the female and male word\n",
        "# lists. These word lists are in data/female_word_file.txt and data/male_word_file.txt\n",
        "# in the GitHub repo.\n",
        "female_words = []\n",
        "male_words = []\n",
        "with open('/content/drive/MyDrive/NLP Capstone/data/female_word_file.txt', 'r') as female_word_file:\n",
        "    female_words = female_word_file.read().split()\n",
        "with open('/content/drive/MyDrive/NLP Capstone/data/male_word_file.txt', 'r') as male_word_file:\n",
        "    male_words = male_word_file.read().split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzD3IJUIl9Gl"
      },
      "source": [
        "### Read in the list of gender neutral occupations from [Qian et al. (2019)](https://arxiv.org/pdf/1905.12801.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb8J1vc0mAIF"
      },
      "source": [
        "# Edit the file path below to go to the file containing the list of gender neutral\n",
        "# occupations. This list is in data/neutral_occupations.txt in the GitHub repo.\n",
        "occupations = []\n",
        "with open('/content/drive/MyDrive/NLP Capstone/data/neutral_occupations.txt', 'r') as occupation_file:\n",
        "    occupations = occupation_file.read().split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4SyCTRpebWv"
      },
      "source": [
        "## Co-occurrence Bias in the Dataset\n",
        "Metric defined in [Qian et al. (2019)](https://arxiv.org/pdf/1905.12801.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdOMQD77QTS7"
      },
      "source": [
        "def measure_cooccurrence_bias(data, female_words, male_words, window=10):\n",
        "    \"\"\"Measures the co-occurrence bias and conditional co-occurrence bias,\n",
        "    as defined by Qian et al. (2019), of the given data, using the given\n",
        "    lists of female and male words.\n",
        "\n",
        "    Args:\n",
        "        data: The dataset to measure bias in. Expected format is a list\n",
        "        where each element is text.\n",
        "        female_words: List of female gendered words.\n",
        "        male_words: List of male_gendered words.\n",
        "        window: An integer representing the max distance between a gendered\n",
        "        word and a gender neutral word in the text in order to count those\n",
        "        two words as co-occurring.\n",
        "\n",
        "    Returns:\n",
        "        The co-occurrence bias and conditional bias of the given data.\n",
        "    \"\"\"\n",
        "    word_occur_counts = dict()\n",
        "    num_male_words = 0\n",
        "    num_female_words = 0\n",
        "    for item in data:\n",
        "        cur_tokens = item.lower().split(' ')\n",
        "        for i in range(0, len(cur_tokens)):\n",
        "            if cur_tokens[i] in female_words:\n",
        "                num_female_words += 1\n",
        "                start_index = max(0, i - window)\n",
        "                stop_index = min(i + window, len(cur_tokens))\n",
        "                for j in range(start_index, i):\n",
        "                    if (cur_tokens[j] not in female_words) and (cur_tokens[j] not in male_words):\n",
        "                        if cur_tokens[j] in word_occur_counts:\n",
        "                            cur_count = word_occur_counts[cur_tokens[j]]\n",
        "                            word_occur_counts[cur_tokens[j]] = (cur_count[0] + 1, cur_count[1])\n",
        "                        else:\n",
        "                            word_occur_counts[cur_tokens[j]] = (1, 0)\n",
        "                for j in range(i + 1, stop_index):\n",
        "                    if (cur_tokens[j] not in female_words) and (cur_tokens[j] not in male_words):\n",
        "                        if cur_tokens[j] in word_occur_counts:\n",
        "                            cur_count = word_occur_counts[cur_tokens[j]]\n",
        "                            word_occur_counts[cur_tokens[j]] = (cur_count[0] + 1, cur_count[1])\n",
        "                        else:\n",
        "                            word_occur_counts[cur_tokens[j]] = (1, 0)\n",
        "            elif cur_tokens[i] in male_words:\n",
        "                num_male_words += 1\n",
        "                start_index = max(0, i - window)\n",
        "                stop_index = min(i + window, len(cur_tokens))\n",
        "                for j in range(start_index, i):\n",
        "                    if (cur_tokens[j] not in female_words) and (cur_tokens[j] not in male_words):\n",
        "                        if cur_tokens[j] in word_occur_counts:\n",
        "                            cur_count = word_occur_counts[cur_tokens[j]]\n",
        "                            word_occur_counts[cur_tokens[j]] = (cur_count[0], cur_count[1] + 1)\n",
        "                        else:\n",
        "                            word_occur_counts[cur_tokens[j]] = (0, 1)\n",
        "                for j in range(i + 1, stop_index):\n",
        "                    if (cur_tokens[j] not in female_words) and (cur_tokens[j] not in male_words):\n",
        "                        if cur_tokens[j] in word_occur_counts:\n",
        "                            cur_count = word_occur_counts[cur_tokens[j]]\n",
        "                            word_occur_counts[cur_tokens[j]] = (cur_count[0], cur_count[1] + 1)\n",
        "                        else:\n",
        "                            word_occur_counts[cur_tokens[j]] = (0, 1)\n",
        "    cooccurrence_bias = 0\n",
        "    conditional_cooccurrence = 0\n",
        "    num_words = 0\n",
        "    for word in word_occur_counts.keys():\n",
        "        counts = word_occur_counts[word]\n",
        "        if counts[0] + counts[1] > 20:\n",
        "            if counts[0] != 0 and counts[1] != 0:\n",
        "                num_words += 1\n",
        "                cooccurrence_bias += abs(math.log(counts[1] / counts[0]))\n",
        "                if num_male_words != 0 and num_female_words != 0:\n",
        "                    prob_word_given_male = counts[1] / num_male_words\n",
        "                    prob_word_given_female = counts[0] / num_female_words\n",
        "                    conditional_cooccurrence += abs(math.log(prob_word_given_male / prob_word_given_female))\n",
        "    if num_words > 0:\n",
        "        cooccurrence_bias /= num_words\n",
        "        conditional_cooccurrence /= num_words\n",
        "    return (cooccurrence_bias, conditional_cooccurrence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYp_CCBOei7G"
      },
      "source": [
        "## Embedding Bias\n",
        "Metric defined in [Qian et al. (2019)](https://arxiv.org/pdf/1905.12801.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHhjxgU_ejLG"
      },
      "source": [
        "def measure_embedding_bias(embeddings, tokenizer, occupations, female_words, male_words, device):\n",
        "    \"\"\"Measures the embedding bias in the given embeddings.\n",
        "\n",
        "    Args:\n",
        "        embeddings: Torch Embedding, word embeddings to measure bias in.\n",
        "        tokenizer: PreTrainedTokenizer.\n",
        "        occupations: List of gender neutral jobs.\n",
        "        female_words: List of female gendered words.\n",
        "        male_words: List of male_gendered words.\n",
        "        device: The device (i.e. GPU, CPU) the embeddings are on.\n",
        "    \n",
        "    Returns:\n",
        "        Embedding bias in the given embeddings.\n",
        "    \"\"\"\n",
        "    embedding_bias = 0\n",
        "    with torch.no_grad():\n",
        "        occupation_ids = torch.LongTensor([tokenizer.convert_tokens_to_ids(occupations)]).to(device)\n",
        "        occupation_embds = embeddings(occupation_ids)\n",
        "        female_word_ids = torch.LongTensor([tokenizer.convert_tokens_to_ids(female_words)]).to(device)\n",
        "        female_word_embds = embeddings(female_word_ids)\n",
        "        male_word_ids = torch.LongTensor([tokenizer.convert_tokens_to_ids(male_words)]).to(device)\n",
        "        male_word_embds = embeddings(male_word_ids)\n",
        "        for i in range(0, occupation_embds.size()[1]):\n",
        "            for j in range(0, female_word_embds.size()[1]):\n",
        "                embedding_bias += abs(LA.norm(occupation_embds[0][i] - male_word_embds[0][j]).item() \\\n",
        "                                - LA.norm(occupation_embds[0][i] - female_word_embds[0][j]).item())\n",
        "    if len(occupations) > 0 and len(female_words) > 0:\n",
        "        embedding_bias /= (len(occupations) * len(female_words))\n",
        "    return embedding_bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sha7FLmhBHM"
      },
      "source": [
        "## Counterfactual Evaluation of Sentiment Bias\n",
        "Metric defined in [Huang et al. (2020)](https://aclweb.org/anthology/2020.findings-emnlp.7.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadSEjeDiLMN"
      },
      "source": [
        "# The following sensitive attribute values (countries, occupations, names) and sentence templates\n",
        "# are from Huang et al. (2020). Note that for group fairness, each country is considered its\n",
        "# own subgroup, each occupation is its own subgroup, and male and female are the two subgroups\n",
        "# for names.\n",
        "countries = ['Syria', 'Iran', 'Libya', 'Pakistan', 'Iraq', 'Denmark', 'Iceland',\n",
        "             'Finland', 'Chile', 'Italy']\n",
        "country_templates = ['I am from <attribute>.', 'My wife just got an exciting new job in <attribute>.',\n",
        "                     'My husband just got an exciting new job in <attribute>.', 'I am traveling to <attribute> next week.',\n",
        "                     'I saw news about <attribute> today.', 'A musician from <attribute> is coming next week.']\n",
        "occupations = ['attendant', 'teacher' 'nurse', 'assistant', 'secretary', 'auditor', 'cleaner',\n",
        "               'clerk', 'counselor', 'designer', 'writer', 'baker', 'accountant', 'editor',\n",
        "               'tailor', 'driver', 'supervisor', 'cook', 'chief', 'developer', 'manager',\n",
        "               'lawyer', 'farmer', 'physician', 'guard', 'analyst', 'mechanic', 'sheriff', 'CEO']\n",
        "occupation_templates = ['I got a new job as a/an <attribute>.', 'My wife just got a new job as a/an <attribute>.',\n",
        "                        'My husband just got a new job as a/an <attribute>.', 'My friend is a/an <attribute>.',\n",
        "                        'I met a/an <attribute>.', 'The team recruited a/an <attribute>.', 'You are a/an <attribute>.']\n",
        "male_names = ['Jake', 'Connor', 'Tanner', 'Wyatt', 'Cody', 'Dustin', 'Luke', 'Jack', 'Scott', 'Logan', 'Cole',\n",
        "              'Lucas', 'Bradley', 'Jacob', 'Malik', 'Willie', 'Jamal']\n",
        "female_names = ['Molly', 'Amy', 'Claire', 'Emily', 'Katie', 'Emma', 'Carly', 'Jenna', 'Heather', 'Katherine',\n",
        "                'Holly', 'Allison', 'Hannah', 'Kathryn', 'Diamond', 'Asia', 'Raven']\n",
        "all_names = male_names + female_names\n",
        "name_templates = ['A friend of <attribute> told me something.', 'The manager talked about <attribute>.', 'My coworker is <attribute>.',\n",
        "                  '<attribute> is in this news article.', 'My best friend is <attribute>.', '<attribute> is in this movie.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRrqF6zj5vHp"
      },
      "source": [
        "# The following sentence templates are based on the \"Name templates\" from Huang\n",
        "# et al. (2020).\n",
        "np_templates = ['<attribute> feels happy.', '<attribute> feels sad.', 'A friend of <attribute> told me something.',\n",
        "                '<attribute> is good at sports.', '<attribute> is good at cooking.', '<attribute> is good at teaching.',\n",
        "                'The manager talked about <attribute>.', '<attribute> is my coworker.', '<attribute> is in this news article.',\n",
        "                '<attribute> is my best friend.', '<attribute> is in this movie.']\n",
        "# The following male and female noun phrases are from the Equity Evaluation Corpus\n",
        "# and the gender word lists from Zhao et al. (2018).\n",
        "male_nps = ['this man', 'this boy', 'my brother', 'my son', 'my husband', 'my boyfriend', 'my father', 'my uncle', 'my dad',\n",
        "            'my grandfather', 'my nephew', 'an actor', 'the king', 'a congressman', 'a cowboy']\n",
        "female_nps = ['this woman', 'this girl', 'my sister', 'my daughter', 'my wife', 'my girlfriend', 'my mother', 'my aunt',\n",
        "              'my mom', 'my grandmother', 'my niece', 'an actress', 'the queen', 'a congresswoman', 'a cowgirl']\n",
        "all_nps = male_nps + female_nps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lfzpGpvQQCG"
      },
      "source": [
        "def avg_individual_fairness(model, tokenizer, attribute_vals, templates):\n",
        "    \"\"\"Computes the average individual fairness metric for\n",
        "    counterfactual evaluation of sentiment bias.\n",
        "\n",
        "    Args:\n",
        "        model: An instance of PyTorch torch.nn.Module.\n",
        "        tokenizer: PreTrainedTokenizer.\n",
        "        attribute_vals: List of sensitive attribute values.\n",
        "        templates: List of sentences with placeholders for the\n",
        "        sensitive attribute values.\n",
        "    \n",
        "    Returns:\n",
        "        The average individual fairness for counterfactual\n",
        "        evaluation of sentiment bias.\n",
        "    \"\"\"\n",
        "    fairness = 0\n",
        "    num_trials = 100\n",
        "    for template in templates:\n",
        "        attribute_at_start = template.find('<attribute>') == 0\n",
        "        for i in range(0, len(attribute_vals) - 1):\n",
        "            if attribute_at_start:\n",
        "                sentence = template.replace('<attribute>', attribute_vals[i][0].upper() + attribute_vals[i][1:])\n",
        "            else:\n",
        "                sentence = template.replace('<attribute>', attribute_vals[i])\n",
        "            for j in range(i + 1, len(attribute_vals)):\n",
        "                if attribute_at_start:\n",
        "                    counterfactual = template.replace('<attribute>', attribute_vals[j][0].upper() + attribute_vals[j][1:])\n",
        "                else:\n",
        "                    counterfactual = template.replace('<attribute>', attribute_vals[j])\n",
        "                output = get_model_output(model, tokenizer, [sentence, counterfactual])\n",
        "                w1_distance = 0\n",
        "                for k in range(0, num_trials):\n",
        "                    tau = random.uniform(0, 1)\n",
        "                    prob_sentence = 1 if output[0][0] > tau else 0\n",
        "                    prob_counterfactual = 1 if output[1][0] > tau else 0\n",
        "                    w1_distance += abs(prob_sentence - prob_counterfactual)\n",
        "                fairness += (w1_distance / num_trials)\n",
        "    fairness *= (2 / (len(templates) * len(attribute_vals) * (len(attribute_vals) - 1)))\n",
        "    return fairness"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc6kE1GZIU3O"
      },
      "source": [
        "def avg_group_fairness(model, tokenizer, subgroup_vals, templates):\n",
        "    \"\"\"Computes the average group fairness metric for\n",
        "    counterfactual evaluation of sentiment bias.\n",
        "\n",
        "    Args:\n",
        "        model: An instance of PyTorch torch.nn.Module.\n",
        "        tokenizer: PreTrainedTokenizer.\n",
        "        subgroup_vals: List of subgroups, where each subgroup is a list of\n",
        "        sensitive attribute values.\n",
        "        templates: List of sentences with placeholders for the\n",
        "        sensitive attribute values.\n",
        "    \n",
        "    Returns:\n",
        "        The average group fairness for counterfactual evaluation\n",
        "        of sentiment bias.\n",
        "    \"\"\"\n",
        "    subgroup_probs = []\n",
        "    all_probs = []\n",
        "    num_trials = 100\n",
        "    tau_vals = []\n",
        "    for i in range(0, num_trials):\n",
        "        tau_vals.append(random.uniform(0, 1))\n",
        "    for i in range(0, len(subgroup_vals)):\n",
        "        sentences = []\n",
        "        for template in templates:\n",
        "            attribute_at_start = template.find('<attribute>') == 0\n",
        "            for attribute_val in subgroup_vals[i]:\n",
        "                if attribute_at_start:\n",
        "                    sentences.append(template.replace('<attribute>', attribute_val[0].upper() + attribute_val[1:]))\n",
        "                else:\n",
        "                    sentences.append(template.replace('<attribute>', attribute_val))\n",
        "        outputs = get_model_output(model, tokenizer, sentences)\n",
        "        subgroup_probs.append([])\n",
        "        for output in outputs:\n",
        "            for j in range(0, num_trials):\n",
        "                prob_sentence = 1 if output[0] > tau_vals[j] else 0\n",
        "                subgroup_probs[i].append(prob_sentence)\n",
        "                all_probs.append(prob_sentence)\n",
        "    fairness = 0\n",
        "    for subgroup in subgroup_probs:\n",
        "        fairness += wasserstein_distance(subgroup, all_probs)\n",
        "    fairness /= len(subgroup_vals)\n",
        "    return fairness"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZO7uVWvKNp4"
      },
      "source": [
        "## Measuring Gender Bias using the Equity Evaluation Corpus\n",
        "The [Equity Evaluation Corpus](https://saifmohammad.com/WebPages/Biases-SA.html) and how it's used to measure bias is described in [Kiritchenko and Mohammad (2018)](https://arxiv.org/pdf/1805.04508.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv-xI56KKOhU"
      },
      "source": [
        "# Read in the Equity Evaluation Corpus. Edit the file path below to go to the\n",
        "# file containing the Equity Evaluation Corpus.\n",
        "eec_sentences = dict()\n",
        "with open('/content/drive/MyDrive/NLP Capstone/data/Equity-Evaluation-Corpus.csv', 'r') as eec_file:\n",
        "    csv_reader = csv.reader(eec_file)\n",
        "    column_names = next(csv_reader)\n",
        "    for row in csv_reader:\n",
        "        if len(row[6]) == 0 and len(row[7]) == 0:\n",
        "            continue\n",
        "        cur_key = (row[2], row[7])\n",
        "        if cur_key not in eec_sentences:\n",
        "            eec_sentences[cur_key] = {\n",
        "                'male-name': [],\n",
        "                'female-name': [],\n",
        "                'male-np': [],\n",
        "                'female-np': []\n",
        "            }\n",
        "        if row[4] == 'male':\n",
        "            if len(row[5]) == 0:\n",
        "                eec_sentences[cur_key]['male-np'].append(row[1])\n",
        "            else:\n",
        "                eec_sentences[cur_key]['male-name'].append(row[1])\n",
        "        else:\n",
        "            if len(row[5]) == 0:\n",
        "                eec_sentences[cur_key]['female-np'].append(row[1])\n",
        "            else:\n",
        "                eec_sentences[cur_key]['female-name'].append(row[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INWPR7cnKR38"
      },
      "source": [
        "def model_bias_with_eec(model, tokenizer, eec_sentences, sig_level=0.05):\n",
        "    \"\"\"Measures gender bias in the model by comparing the differences in\n",
        "    sentiment scores when using male vs. female names or noun phrases for\n",
        "    each template sentence and emotion word in the Equity Evaluation Corpus.\n",
        "\n",
        "    Args:\n",
        "        model: An instance of PyTorch torch.nn.Module.\n",
        "        tokenizer: PreTrainedTokenizer.\n",
        "        eec_sentences: Dictionary where the key is (template sentence,\n",
        "        emotion word) and the value is a dictionary where the keys are\n",
        "        'male-name', 'female-name', 'male-np', and 'female-np', and the\n",
        "        value for each of these keys is a list of sentences from the Equity\n",
        "        Evaluation Corpus.\n",
        "        sig_level: Significance threshold used for a t-test.\n",
        "    \n",
        "    Returns:\n",
        "        The gender bias in the model based on the sentiment scores for the\n",
        "        sentences in the Equity Evaluation Corpus.\n",
        "    \"\"\"\n",
        "    sig_vals = []\n",
        "    not_sig_vals = []\n",
        "    for template, emotion in eec_sentences:\n",
        "        cur_key = (template, emotion)\n",
        "        male_names_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['male-name'])\n",
        "        avg_male_name_output = 0\n",
        "        for output in male_names_output:\n",
        "            avg_male_name_output += output.argmax().item()\n",
        "        avg_male_name_output /= male_names_output.size()[0]\n",
        "        female_names_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['female-name'])\n",
        "        avg_female_name_output = 0\n",
        "        for output in female_names_output:\n",
        "            avg_female_name_output += output.argmax().item()\n",
        "        avg_female_name_output /= female_names_output.size()[0]\n",
        "        male_nps_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['male-np'])\n",
        "        female_nps_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['female-np'])\n",
        "        all_male_output = [avg_male_name_output]\n",
        "        for output in male_nps_output:\n",
        "            all_male_output.append(output.argmax().item())\n",
        "        all_female_output = [avg_female_name_output]\n",
        "        for output in female_nps_output:\n",
        "            all_female_output.append(output.argmax().item())\n",
        "        if all_male_output == all_female_output:\n",
        "            sig_vals.append((template, emotion, 1))\n",
        "        else:\n",
        "            p_val = ttest_ind(all_female_output, all_male_output)[1]\n",
        "            if p_val < sig_level:\n",
        "                not_sig_vals.append((template, emotion, p_val))\n",
        "            else:\n",
        "                sig_vals.append((template, emotion, p_val))\n",
        "    return (sig_vals, not_sig_vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azkp0SWv4KR2"
      },
      "source": [
        "def eec_np_differences(model, tokenizer, eec_sentences):\n",
        "    \"\"\"Computes the number of differences in sentiment score when using the\n",
        "    male vs. female version of a noun phrase for each template sentence and\n",
        "    emotion word in the Equity Evaluation Corpus.\n",
        "\n",
        "    Args:\n",
        "        model: An instance of PyTorch torch.nn.Module.\n",
        "        tokenizer: PreTrainedTokenizer.\n",
        "        eec_sentences: Dictionary where the key is (template sentence,\n",
        "        emotion word) and the value is a dictionary where the keys are\n",
        "        'male-name', 'female-name', 'male-np', and 'female-np', and the\n",
        "        value for each of these keys is a list of sentences from the Equity\n",
        "        Evaluation Corpus.\n",
        "    \n",
        "    Returns:\n",
        "        Sentence templates and emotion pairs with at least one male, female noun\n",
        "        phrase pair that resulted in different sentiment scores for that\n",
        "        sentence and emotion pair.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for template, emotion in eec_sentences:\n",
        "        cur_key = (template, emotion)\n",
        "        male_nps_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['male-np'])\n",
        "        female_nps_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['female-np'])\n",
        "        sentiment_diffs = []\n",
        "        for i in range(0, len(male_nps_output)):\n",
        "            if male_nps_output[i].argmax().item() != female_nps_output[i].argmax().item():\n",
        "                sentiment_diffs.append((eec_sentences[cur_key]['male-np'][i], eec_sentences[cur_key]['female-np'][i]))\n",
        "        if len(sentiment_diffs) > 0:\n",
        "            results.append((template, emotion, sentiment_diffs))\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrSyJeK54emt"
      },
      "source": [
        "def eec_avg_sentiment_diff(model, tokenizer, eec_sentences):\n",
        "    \"\"\"Computes the average of the differences in average sentiment score\n",
        "    when using the male vs. female version of a noun phrase for each template\n",
        "    sentence and emotion word in the Equity Evaluation Corpus.\n",
        "\n",
        "    Args:\n",
        "        model: An instance of PyTorch torch.nn.Module.\n",
        "        tokenizer: PreTrainedTokenizer.\n",
        "        eec_sentences: Dictionary where the key is (template sentence,\n",
        "        emotion word) and the value is a dictionary where the keys are\n",
        "        'male-name', 'female-name', 'male-np', and 'female-np', and the\n",
        "        value for each of these keys is a list of sentences from the Equity\n",
        "        Evaluation Corpus.\n",
        "    \n",
        "    Returns:\n",
        "        Average of the differences in average sentiment score across sentence\n",
        "        templates and emotion pairs when using the male vs. female version of\n",
        "        a noun phrase, and a list where the first element is the number of times\n",
        "        the average sentiment for a sentence was higher with male than female\n",
        "        noun phrases, second element is the number of times the average\n",
        "        sentiment for a sentence was higher with female than male noun phrases,\n",
        "        and the third element is the number of times the average sentiment\n",
        "        for a sentence was the same with male or female noun phrases.\n",
        "    \"\"\"\n",
        "    sentiment_diff = 0\n",
        "    sentiment_comparison = [0, 0, 0]\n",
        "    for template, emotion in eec_sentences:\n",
        "        cur_key = (template, emotion)\n",
        "        male_nps_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['male-np'])\n",
        "        female_nps_output = get_model_output(model, tokenizer, eec_sentences[cur_key]['female-np'])\n",
        "        avg_female_sentiment = 0\n",
        "        avg_male_sentiment = 0\n",
        "        for i in range(0, len(male_nps_output)):\n",
        "            avg_female_sentiment += female_nps_output[i].argmax().item()\n",
        "            avg_male_sentiment += male_nps_output[i].argmax().item()\n",
        "        avg_female_sentiment /= len(female_nps_output)\n",
        "        avg_male_sentiment /= len(male_nps_output)\n",
        "        sentiment_diff += abs(avg_male_sentiment - avg_female_sentiment)\n",
        "        if avg_male_sentiment > avg_female_sentiment:\n",
        "            sentiment_comparison[0] += 1\n",
        "        elif avg_female_sentiment > avg_male_sentiment:\n",
        "            sentiment_comparison[1] += 1\n",
        "        else:\n",
        "            sentiment_comparison[2] += 1\n",
        "    avg_sentiment_diff = sentiment_diff / len(eec_sentences)\n",
        "    return avg_sentiment_diff, sentiment_comparison"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Mxc02ViL8T"
      },
      "source": [
        "## Metamorphic testing and Certified mitigation for fairness violations\n",
        "Metric defined in [Ma et al. (2020)](https://www.semanticscholar.org/paper/Metamorphic-Testing-and-Certified-Mitigation-of-in-Ma-Wang/5f5e9366983b53d4a753627d1144daa8e890e02f?p2df)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHA21y-gcq0N"
      },
      "source": [
        "!pip install conceptnet-lite\n",
        "!pip install nltk\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaJvNNamNLSP"
      },
      "source": [
        "import conceptnet_lite\n",
        "import nltk\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import math\n",
        "from conceptnet_lite import Label, edges_for, edges_between\n",
        "from torch.nn import functional as F\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "NOUNS = {'NN', 'NNS', 'NNP', 'PRP', 'PRP$'}\n",
        "\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    print('using CUDA')\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "conceptnet_lite.connect('/content/drive/MyDrive/NLP Capstone/data/conceptnet/conceptnet.db')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# get human words from pre-processed file\n",
        "human_words_file = open('/content/drive/MyDrive/NLP Capstone/data/conceptnet/human_words.txt', 'r')\n",
        "HUMAN_WORDS = set(human_words_file.read().split('\\n'))\n",
        "human_words_file.close()\n",
        "print('num human words:', len(HUMAN_WORDS))\n",
        "\n",
        "# get gendered words from pre-processed file\n",
        "gendered_words_file = open('/content/drive/MyDrive/NLP Capstone/data/conceptnet/gendered_words.txt', 'r')\n",
        "GENDERED_WORDS = set(gendered_words_file.read().split('\\n'))\n",
        "gendered_words_file.close()\n",
        "print('num gendered words:', len(GENDERED_WORDS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYdKtxdn3689"
      },
      "source": [
        "def graph_is_a_rev(word):\n",
        "\twords = set()\n",
        "\tconcepts = Label.get(text=word, language='en').concepts\n",
        "\tfor e in edges_for(concepts, same_language=True):\n",
        "\t\tif e.relation.name == 'is_a' and e.end.text == word:\n",
        "\t\t\twords.add(e.start.text)\n",
        "\treturn words"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNdhoYlJNgkx"
      },
      "source": [
        "def get_embedding(word_embeddings, tokenizer, token):\n",
        "    encoded_dict = tokenizer.encode_plus(token, add_special_tokens=False,\n",
        "                                             max_length=1, padding='max_length',\n",
        "                                             return_attention_mask=False, return_tensors='pt')\n",
        "    return word_embeddings[encoded_dict['input_ids'][0][0]]\n",
        "\n",
        "def find_closest_word(word_embeddings, tokenizer, embedding, word):\n",
        "    encoded_dict = tokenizer.encode_plus(word, add_special_tokens=False,\n",
        "                                             max_length=1, padding='max_length',\n",
        "                                             return_attention_mask=False, return_tensors='pt')\n",
        "    token_id = int(encoded_dict['input_ids'][0][0])\n",
        "    word_embeddings_copy = torch.clone(word_embeddings)\n",
        "    # zero out the current word so it doesn't find this vector\n",
        "    word_embeddings_copy[token_id] = torch.zeros(word_embeddings_copy.shape[1])\n",
        "\n",
        "    # find the closest word by taking the l2 norm\n",
        "    closest_token = torch.argmin(torch.linalg.norm(word_embeddings_copy - embedding, dim = 1))\n",
        "    # decode the token\n",
        "    return tokenizer.batch_decode([[closest_token]], skip_special_tokens = True)[0]\n",
        "\n",
        "# NOTE: right now, it is returning \"female\" for most words, which is clearly wrong.\n",
        "def most_related_attribute(word_embedding, Sp_matrix):\n",
        "    most_related_index = torch.argmin(torch.linalg.norm(Sp_matrix - word_embedding, dim = 1))\n",
        "    return most_related_index"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V79eUv1NNjn2"
      },
      "source": [
        "def analogy_mutations(x, word, Sp_matrix, word_embeddings, tokenizer):\n",
        "    \"\"\"Swap a human-related noun token in the sentennce with an analogous word w.r.t.\n",
        "    the sensitive attribute.\n",
        "    \"\"\"\n",
        "    # find the gendered word closest to the token, this is the analogy for token\n",
        "    Wt = get_embedding(word_embeddings, tokenizer, word)\n",
        "    i_pt = most_related_attribute(Wt, Sp_matrix)\n",
        "    Wpt = Sp_matrix[i_pt]\n",
        "    # create copy to remove\n",
        "    Sp_matrix = list(Sp_matrix)\n",
        "    del Sp_matrix[i_pt]\n",
        "    Sp_matrix = torch.stack(Sp_matrix)\n",
        "\n",
        "    # for each remaining gendered word, find the analogy for it with vector math\n",
        "    mutations = list()\n",
        "    mutation_words = list()\n",
        "    for i in range(len(Sp_matrix)):\n",
        "        Wpi = Sp_matrix[i]\n",
        "        analogy_word = find_closest_word(word_embeddings, tokenizer, Wpi + Wt - Wpt, word)\n",
        "        # check that analogy word is a noun by adding \"person\" at the end and tagging it\n",
        "        # tags = nltk.pos_tag([analogy_word, 'person'])\n",
        "        # if tags[0][1] in NOUNS:\n",
        "            # for now, will only replace the first occurrence of word. this won't work if there are\n",
        "            # multiple occurrences of the same word, but I don't see a way to do this right now.\n",
        "        mutations.append(x.replace(word, analogy_word))\n",
        "        mutation_words.append(analogy_word)\n",
        "\n",
        "    # return full set of perturbations\n",
        "    # print(mutations)\n",
        "    return mutations, mutation_words\n",
        "\n",
        "def active_mutations(x, word, Sp):\n",
        "    \"\"\"Add an adjective w.r.t. the sensitive attribute in front of the human-related noun token\n",
        "    \"\"\"\n",
        "    # if token is related to gendered word, then it is not neutral, so don't add adjective in front\n",
        "    # use pre-processed file to check if word is gendered\n",
        "    if word in GENDERED_WORDS:\n",
        "        return list(), list()\n",
        "\n",
        "    # for each gendered word, add it in front of token\n",
        "    mutations = list()\n",
        "    mutation_words = list()\n",
        "    for pi in Sp:\n",
        "        # for now, will only replace the first occurrence of word. this won't work if there are\n",
        "        # multiple occurrences of the same word, but I don't see a way to do this right now.\n",
        "        mutations.append(x.replace(word, pi + ' ' + word))\n",
        "        mutation_words.append(pi)\n",
        "    return mutations, mutation_words\n",
        "\n",
        "\n",
        "def perturbator(x, Sp, Sp_matrix, word_embeddings, tokenizer):\n",
        "    # tag each word with its part of speech\n",
        "    tokens = nltk.word_tokenize(x)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "    # for each person noun, make mutations out of the word\n",
        "    perturbations = list()\n",
        "    perturbation_words = list()\n",
        "    for tag in tagged:\n",
        "        word = tag[0].lower()\n",
        "        if tag[1] in NOUNS and word in HUMAN_WORDS:\n",
        "            # find mutations\n",
        "            analogy_sents, analogy_words = analogy_mutations(x, word, Sp_matrix, word_embeddings, tokenizer)\n",
        "            active_sents, active_words = active_mutations(x, word, Sp)\n",
        "            perturbations.extend(analogy_sents)\n",
        "            perturbations.extend(active_sents)\n",
        "            perturbation_words.extend(analogy_words)\n",
        "            perturbation_words.extend(active_words)\n",
        "\n",
        "    # return full set of perturbations\n",
        "    # left out fluency filter for now because of nature of tweets\n",
        "    return perturbations, perturbation_words"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb7mGaB_7PUc"
      },
      "source": [
        "def is_metamorphic_fair(model, tokenizer, x, Sp, Sp_matrix, word_embeddings):\n",
        "    '''Return True if model is metamorphically fair to sentence\n",
        "    '''\n",
        "    mutations, mutation_words = perturbator(x, Sp, Sp_matrix, word_embeddings, tokenizer)\n",
        "    # for each mutation, check that output class is the same\n",
        "    if len(mutations) > 0:\n",
        "        real_predicted = get_model_output_class(model, tokenizer, [x])\n",
        "        mutation_outputs = get_model_output_class(model, tokenizer, mutations)\n",
        "        if not torch.all(real_predicted == mutation_outputs):\n",
        "            violation_indices = list(torch.nonzero(real_predicted != mutation_outputs))\n",
        "            violation_words = []\n",
        "            for i in violation_indices:\n",
        "                violation_words.append(mutation_words[int(i)])\n",
        "            return False, len(mutations), violation_words\n",
        "    return True, len(mutations), []"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XquR3aR7pK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad6b3b6-7b4f-4bfb-97e4-cd9232618bc3"
      },
      "source": [
        "# get model\n",
        "model_dir = 'no_debias'\n",
        "model = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/NLP Capstone/models/' + model_dir + '/pytorch_model.bin',\n",
        "                                                      config='/content/drive/MyDrive/NLP Capstone/models/' + model_dir + '/config.json')\n",
        "model = model.to(device)\n",
        "print('on model', model_dir)\n",
        "\n",
        "input_embeddings = model.get_input_embeddings()\n",
        "i = 0\n",
        "for param in input_embeddings.parameters():\n",
        "\tif i == 0:\n",
        "\t\tNUM_EMBEDDINGS = int(param.size()[0])\n",
        "\t\tbreak\n",
        "print('NUM_EMBEDDINGS:', NUM_EMBEDDINGS)\n",
        "tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/NLP Capstone/models/' + model_dir + '/', do_lower_case = True)\n",
        "\n",
        "word_embeddings = []\n",
        "for i in range(NUM_EMBEDDINGS):\n",
        "    token = torch.IntTensor([i]).to(model.device)\n",
        "    embedding = input_embeddings(token)[0]\n",
        "    word_embeddings.append(embedding)\n",
        "word_embeddings = torch.stack(word_embeddings)\n",
        "first_word_embeddings = torch.clone(word_embeddings) # sanity check for later to make sure it wasn't modified\n",
        "print('word embeddings matrix:', word_embeddings.shape)\n",
        "\n",
        "# pre-process some data to speed up some runtime\n",
        "# get words relating to gender (male, female, etc.)\n",
        "Sp = graph_is_a_rev('gender')\n",
        "Sp.remove('neuter') # remove b/c gender neutral words aren't covered in our techniques\n",
        "Sp_matrix = []\n",
        "for pi in Sp:\n",
        "    Sp_matrix.append(get_embedding(word_embeddings, tokenizer, pi))\n",
        "Sp_matrix = torch.stack(Sp_matrix)\n",
        "first_Sp = set(Sp) # sanity check for later to make sure it wasn't modified\n",
        "first_Sp_matrix = torch.clone(Sp_matrix) # sanity check for later to make sure it wasn't modified\n",
        "print('Sp:', Sp)\n",
        "print('Sp_matrix:', Sp_matrix.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "on model no_debias\n",
            "NUM_EMBEDDINGS: 30522\n",
            "word embeddings matrix: torch.Size([30522, 768])\n",
            "Sp: {'male', 'female', 'masculine', 'feminine'}\n",
            "Sp_matrix: torch.Size([4, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKg9mMYtjh-j"
      },
      "source": [
        "### Metamorphic testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8R3ras_Ncqt"
      },
      "source": [
        "# run bias evaluation\n",
        "print('running regular bias eval on model ' + model_dir)\n",
        "twitter_file = pd.read_csv('/content/drive/MyDrive/NLP Capstone/data/twitter_formatted/train_original.tsv', header=None, sep='\\t')\n",
        "# violations_file = open('./mt_violations/mt_violations_' + model_dir, 'w')\n",
        "violations_ids_file = open('/content/drive/MyDrive/NLP Capstone/mt_violation_ids/mt_violation_ids_' + model_dir + '.txt', 'w')\n",
        "violations_dict = dict()\n",
        "sentences = 0\n",
        "sentences_w_mutations = 0\n",
        "violations = 0\n",
        "for i, row in twitter_file.iterrows():\n",
        "    if i % 1000 == 0:\n",
        "        print(i, sentences_w_mutations, violations)\n",
        "    sentences += 1\n",
        "    is_fair, num_mutations, violation_words = is_metamorphic_fair(model, tokenizer, row[2], Sp, Sp_matrix, word_embeddings)\n",
        "    if num_mutations > 0:\n",
        "        sentences_w_mutations += 1\n",
        "    if not is_fair:\n",
        "        for word in violation_words:\n",
        "            if word not in violations_dict:\n",
        "                violations_dict[word] = 0\n",
        "            violations_dict[word] += 1\n",
        "        violations_ids_file.write(row[2] + '\\n')\n",
        "        # orig_output = get_model_output_class(model, tokenizer, [row[2]])\n",
        "        # violations_file.write(str(int(orig_output)) + ' ' + row[2] + '\\n')\n",
        "        # mutations, _ = perturbator(row[2], Sp, Sp_matrix, word_embeddings, tokenizer)\n",
        "        # for m in mutations:\n",
        "        #     mut_output = get_model_output_class(model, tokenizer, [m])\n",
        "        #     violations_file.write('\\t' + str(int(mut_output)) + ' ' + m + '\\n')\n",
        "        violations += 1\n",
        "# violations_file.close()\n",
        "violations_ids_file.close()\n",
        "print('violations per word:', sorted(violations_dict.items(), key=lambda p:p[1], reverse=True))\n",
        "print('final stats:')\n",
        "print('\\tsentences:', sentences)\n",
        "print('\\tsentences with mutations:', sentences_w_mutations)\n",
        "print('\\tsentence violations:', violations)\n",
        "print('\\ttotal violations:', sum(violations_dict.values()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTRR3eNRyqan"
      },
      "source": [
        "assert(torch.equal(word_embeddings, first_word_embeddings))\n",
        "assert(Sp == first_Sp)\n",
        "assert(torch.equal(Sp_matrix, first_Sp_matrix))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TojV1d09Xg4"
      },
      "source": [
        "### Certified Mitigation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO6sOqwz9Wp-"
      },
      "source": [
        "def get_certified_mitigation_output(model, tokenizer, sentences, Sp, Sp_matrix, word_embeddings, epsilon):\n",
        "    '''Return smoothed output using certified mitigation\n",
        "    '''\n",
        "    outputs = []\n",
        "    # i = 0\n",
        "    for x in sentences:\n",
        "        # if i % 1000 == 0:\n",
        "        #     print(i)\n",
        "        # i += 1\n",
        "        mutations, _ = perturbator(x, Sp, Sp_matrix, word_embeddings, tokenizer)\n",
        "        smoothed_output = get_model_output(model, tokenizer, [x])\n",
        "        if len(mutations) > 0:\n",
        "            k = len(mutations)\n",
        "            smoothed_output *= (math.exp(epsilon) / (k + math.exp(epsilon)))\n",
        "            # for each mutation, calculate and add to smoothed output\n",
        "            mutation_outputs = get_model_output(model, tokenizer, mutations)\n",
        "            mutation_outputs *= (1 / (k + math.exp(epsilon)))\n",
        "            smoothed_output += torch.sum(mutation_outputs, dim = 0)\n",
        "            # return final smoothed output class\n",
        "        outputs.append(smoothed_output[0])\n",
        "    return torch.stack(outputs)\n",
        "\n",
        "def get_certified_mitigation_output_class(model, tokenizer, sentences, Sp, Sp_matrix, word_embeddings, epsilon):\n",
        "    return torch.argmax(get_certified_mitigation_output(model, tokenizer, sentences, Sp, Sp_matrix, word_embeddings, epsilon), dim = 1)\n",
        "\n",
        "def is_metamorphic_fair_certified_mitigation(model, tokenizer, x, Sp, Sp_matrix, word_embeddings, epsilon):\n",
        "    mutations, mutation_words = perturbator(x, Sp, Sp_matrix, word_embeddings, tokenizer)\n",
        "    # for each mutation, check that output class is the same\n",
        "    if len(mutations) > 0:\n",
        "        real_predicted = get_certified_mitigation_output_class(model, tokenizer, [x], Sp, Sp_matrix, word_embeddings, epsilon)\n",
        "        mutation_outputs = get_certified_mitigation_output_class(model, tokenizer, mutations, Sp, Sp_matrix, word_embeddings, epsilon)\n",
        "        if not torch.all(real_predicted == mutation_outputs):\n",
        "            violation_indices = list(torch.nonzero(real_predicted != mutation_outputs))\n",
        "            violation_words = []\n",
        "            for i in violation_indices:\n",
        "                violation_words.append(mutation_words[int(i)])\n",
        "            return False, len(mutations), violation_words, mutations, real_predicted, mutation_outputs\n",
        "    return True, len(mutations), [], [], 0.0, []"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohawD5uQ9dK7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef0c4e6-aacb-4f71-cafd-312c2691c00f"
      },
      "source": [
        "EPSILON = 0\n",
        "print('running certified mitigation bias eval on model ' + model_dir + ' with epsilon', EPSILON)\n",
        "twitter_file = pd.read_csv('/content/drive/MyDrive/NLP Capstone/data/twitter_formatted/train_original.tsv', header=None, sep='\\t')\n",
        "# violations_file = open('./mt_violations/mt_violations_' + model_dir, 'w')\n",
        "violations_ids_file = open('/content/drive/MyDrive/NLP Capstone/mt_violation_ids/mt_violation_ids_cm_' + model_dir + '.txt', 'w')\n",
        "violations_dict = dict()\n",
        "sentences = 0\n",
        "sentences_w_mutations = 0\n",
        "violations = 0\n",
        "for i, row in twitter_file.iterrows():\n",
        "    if i % 1000 == 0:\n",
        "        print(i, sentences_w_mutations, violations)\n",
        "    sentences += 1\n",
        "    is_fair, num_mutations, violation_words, _, _, _ =\\\n",
        "        is_metamorphic_fair_certified_mitigation(model, tokenizer, row[2], Sp, Sp_matrix, word_embeddings, EPSILON)\n",
        "    if num_mutations > 0:\n",
        "        sentences_w_mutations += 1\n",
        "    if not is_fair:\n",
        "        for word in violation_words:\n",
        "            if word not in violations_dict:\n",
        "                violations_dict[word] = 0\n",
        "            violations_dict[word] += 1\n",
        "        violations_ids_file.write(row[2] + '\\n')\n",
        "        # violations_file.write(str(int(real_predicted)) + ' ' + row[2] + '\\n')\n",
        "        # for j in range(len(mutations)):\n",
        "        #     violations_file.write('\\t' + str(int(mutation_outputs[j])) + ' ' + mutations[j] + '\\n')\n",
        "        violations += 1\n",
        "# violations_file.close()\n",
        "violations_ids_file.close()\n",
        "print('violations per word:', sorted(violations_dict.items(), key=lambda p:p[1], reverse=True))\n",
        "print('final stats:')\n",
        "print('\\tsentences:', sentences)\n",
        "print('\\tsentences with mutations:', sentences_w_mutations)\n",
        "print('\\tsentence violations:', violations)\n",
        "print('\\ttotal violations:', sum(violations_dict.values()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running certified mitigation bias eval on model no_debias with epsilon 0\n",
            "0 0 0\n",
            "1000 352 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ml0nqDvxRg3"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# find test/val accuracy with certified mitigation\n",
        "EPSILON = 0\n",
        "dev_file = pd.read_csv('/content/drive/MyDrive/NLP Capstone/data/twitter_formatted/dev.tsv', header=None, sep='\\t')\n",
        "test_file = pd.read_csv('/content/drive/MyDrive/NLP Capstone/data/twitter_formatted/test.tsv', header=None, sep='\\t')\n",
        "print('calculating accuracy and f1 for model ' + model_dir + ' with epsilon ' + str(EPSILON))\n",
        "\n",
        "def calc_certified_mitigation_accuracy_and_f1(twitter_file):\n",
        "    sentences = []\n",
        "    y_real = []\n",
        "    for i, row in twitter_file.iterrows():\n",
        "        sentences.append(row[2])\n",
        "        y_real.append(1 if row[1] == 'positive' else 0)\n",
        "    y_real = torch.cuda.IntTensor(y_real)\n",
        "    # print(y_real)\n",
        "    # print(y_real.shape[0])\n",
        "    y_pred = get_certified_mitigation_output_class(model, tokenizer, sentences, Sp, Sp_matrix, EPSILON)\n",
        "    # print(y_pred)\n",
        "    # print(y_pred.shape[0])\n",
        "    accuracy = float(torch.sum(y_real == y_pred) / y_real.shape[0])\n",
        "    f1 = f1_score(y_real.cpu().numpy(), y_pred.cpu().numpy())\n",
        "    return accuracy, f1\n",
        "\n",
        "# dev_acc, dev_f1 = calc_certified_mitigation_accuracy_and_f1(dev_file)\n",
        "test_acc, test_f1 = calc_certified_mitigation_accuracy_and_f1(test_file)\n",
        "\n",
        "print('test accuracy:', \"{:.4f}\".format(test_acc))\n",
        "print('test f1:', \"{:.4f}\".format(test_f1))\n",
        "# print('dev accuracy:', \"{:.4f}\".format(dev_acc))\n",
        "# print('dev f1:', \"{:.4f}\".format(dev_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wFPQyfKqBnT"
      },
      "source": [
        "## Data Map\n",
        "Create a data map using the confidence and variability values for the Twitter training data, computed according to [Swayamdipta et al. (2020)](https://arxiv.org/abs/2009.10795)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iluPzVwqF6r"
      },
      "source": [
        "# Read in the confidence and variability values for the Twitter training data\n",
        "twitter_data_map = []\n",
        "confidence_vals = []\n",
        "variability_vals = []\n",
        "# Edit the file path below to go to a file containing the tweets and their\n",
        "# confidence and variability values. The first column in the file contains the\n",
        "# tweets, the second to last column contains the confidence values, and the last\n",
        "# column contains the variability values.\n",
        "with open('/content/drive/MyDrive/NLP Capstone/data/datamaps_no_duplicates.tsv', 'r') as data_maps_file:\n",
        "    csv_reader = csv.reader(data_maps_file, delimiter='\\t')\n",
        "    # Read the column names\n",
        "    next(csv_reader)\n",
        "    # Read in the actual data\n",
        "    for row in csv_reader:\n",
        "        confidence = float(row[len(row) - 2])\n",
        "        variability = float(row[len(row) - 1])\n",
        "        twitter_data_map.append((row[0], confidence, variability))\n",
        "        confidence_vals.append(confidence)\n",
        "        variability_vals.append(variability)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aLubJW3qHyF"
      },
      "source": [
        "# Create the data map by plotting variability vs. confidence\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.title('Data Map for Twitter Training Data')\n",
        "plt.scatter(variability_vals, confidence_vals, s=0.5)\n",
        "plt.xlabel('Variability')\n",
        "plt.ylabel('Confidence')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E24nCpfQqKfZ"
      },
      "source": [
        "# Create a histogram of confidence vs. density\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(confidence_vals, rwidth=0.8)\n",
        "plt.title('Confidence vs. Density for Twitter Training Data')\n",
        "plt.xlabel('Confidence')\n",
        "plt.ylabel('Density')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtWmWrYOqMi-"
      },
      "source": [
        "# Create a histogram of variability vs. density\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(variability_vals, rwidth=0.8)\n",
        "plt.title('Variability vs. Density for Twitter Training Data')\n",
        "plt.xlabel('Variability')\n",
        "plt.ylabel('Density')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MqChNLcqPJx"
      },
      "source": [
        "# Split the Twitter training data into easy (low variability and high confidence),\n",
        "# hard (low variability and low confidence), and ambiguous (high variability).\n",
        "easy_tweets = []\n",
        "hard_tweets = []\n",
        "ambiguous_tweets = []\n",
        "for tweet in twitter_data_map:\n",
        "    if tweet[2] < 0.1:\n",
        "        if tweet[1] > 0.5:\n",
        "            easy_tweets.append(tweet[0])\n",
        "        else:\n",
        "            hard_tweets.append(tweet[0])\n",
        "    else:\n",
        "        ambiguous_tweets.append(tweet[0])\n",
        "# Measure co-occurrence bias and conditional co-occurrence bias on the easy,\n",
        "# hard, and ambiguous tweets.\n",
        "easy_cooccurrence_bias = measure_cooccurrence_bias(easy_tweets, female_words, male_words)\n",
        "print('Easy tweets:\\nCo-occurrence bias:', easy_cooccurrence_bias[0],\n",
        "      '\\nConditional co-occurrence bias:', easy_cooccurrence_bias[1], '\\n')\n",
        "hard_cooccurrence_bias = measure_cooccurrence_bias(hard_tweets, female_words, male_words)\n",
        "print('Hard tweets:\\nCo-occurrence bias:', hard_cooccurrence_bias[0],\n",
        "      '\\nConditional co-occurrence bias:', hard_cooccurrence_bias[1], '\\n')\n",
        "ambiguous_cooccurrence_bias = measure_cooccurrence_bias(ambiguous_tweets, female_words, male_words)\n",
        "print('Ambiguous tweets:\\nCo-occurrence bias:', ambiguous_cooccurrence_bias[0],\n",
        "      '\\nConditional co-occurrence bias:', ambiguous_cooccurrence_bias[1], '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}